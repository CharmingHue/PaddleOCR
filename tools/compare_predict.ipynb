{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def CompareResult(annotation_file, prediction_file, output_file):\n",
    "    # 加载真实的标签数据\n",
    "    true_labels = {}\n",
    "    with open(annotation_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            # 从文件路径中提取基本的文件名用于匹配\n",
    "            filename = data['filename'].split('/')[-1]\n",
    "            true_labels[filename] = data['text'].lower()  # 确保大小写一致\n",
    "\n",
    "    # 加载预测的结果数据\n",
    "    with open(prediction_file, 'r', encoding='utf-8') as f, open(output_file, 'w', encoding='utf-8') as output_f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if not parts:\n",
    "                continue\n",
    "            # 从文件路径中提取基本的文件名用于匹配\n",
    "            filename = parts[0].split('/')[-1]\n",
    "            predicted_text = parts[1].lower()  # 确保大小写一致\n",
    "            confidence = float(parts[2])\n",
    "\n",
    "            # 比较预测结果和真实标签\n",
    "            if filename in true_labels:\n",
    "                if true_labels[filename] != predicted_text:\n",
    "                    diff_text = f\"Filename: {filename}\\nTrue Label:\\t{true_labels[filename]}\\nPredicted:\\t{predicted_text}, Confidence: {confidence}\\n\\n\"\n",
    "                    print(diff_text)\n",
    "                    output_f.write(diff_text)\n",
    "    # 注意替换 'your_annotation_file.jsonl' 和 'your_prediction_file.txt'\n",
    "    # 为你的文件路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cloformer_cppd_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=['IC13', 'SVT', 'IIIT5K', 'IC15', 'SVTP', 'CUTE80']\n",
    "\n",
    "for dataset in datasets:\n",
    "    annotation_file = f'train_data/common_benchmarks/{dataset}/annotation.jsonl'\n",
    "    prediction_file = f'output/rec/rec_cloformer_cppd_base/rec_cloformer_cppd_base_{dataset}.txt'\n",
    "    output_file = f'output/rec/rec_cloformer_cppd_base/diff_cloformer_cppd_base_{dataset}.txt'\n",
    "    \n",
    "    CompareResult(annotation_file, prediction_file, output_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## svtr_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=['IC13', 'SVT', 'IIIT5K', 'IC15', 'SVTP', 'CUTE80']\n",
    "\n",
    "for dataset in datasets:\n",
    "    annotation_file = f'train_data/common_benchmarks/{dataset}/annotation.jsonl'\n",
    "    prediction_file = f'output/rec/rec_svtr_base_none_ctc_en_train/predicts_svtr_base_{dataset}.txt'\n",
    "    output_file = f'output/rec/rec_svtr_base_none_ctc_en_train/diff_predicts_svtr_base_{dataset}.txt'\n",
    "    \n",
    "    CompareResult(annotation_file, prediction_file, output_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## r45_abinet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=['IC13', 'SVT', 'IIIT5K', 'IC15', 'SVTP', 'CUTE80']\n",
    "\n",
    "for dataset in datasets:\n",
    "    annotation_file = f'train_data/common_benchmarks/{dataset}/annotation.jsonl'\n",
    "    prediction_file = f'output/rec/rec_r45_abinet_train/predicts_r45_abinet_{dataset}.txt'\n",
    "    output_file = f'output/rec/rec_r45_abinet_train/diff_predicts_r45_abinet_{dataset}.txt'\n",
    "    \n",
    "    CompareResult(annotation_file, prediction_file, output_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## r45_visionlan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=['IC13', 'SVT', 'IIIT5K', 'IC15', 'SVTP', 'CUTE80']\n",
    "\n",
    "for dataset in datasets:\n",
    "    annotation_file = f'train_data/common_benchmarks/{dataset}/annotation.jsonl'\n",
    "    prediction_file = f'output/rec/rec_r45_visionlan_train/predicts_r45_visionlan_{dataset}.txt'\n",
    "    output_file = f'output/rec/rec_r45_visionlan_train/diff_predicts_r45_visionlan_{dataset}.txt'\n",
    "    \n",
    "    CompareResult(annotation_file, prediction_file, output_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vit_parseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=['IC13', 'SVT', 'IIIT5K', 'IC15', 'SVTP', 'CUTE80']\n",
    "\n",
    "for dataset in datasets:\n",
    "    annotation_file = f'train_data/common_benchmarks/{dataset}/annotation.jsonl'\n",
    "    prediction_file = f'output/rec/rec_vit_parseq_train/predicts_vit_parseq_{dataset}.txt'\n",
    "    output_file = f'output/rec/rec_vit_parseq_train/diff_predicts_vit_parseq_{dataset}.txt'\n",
    "    \n",
    "    CompareResult(annotation_file, prediction_file, output_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diff common six benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def diff_all(dataset):\n",
    "    # 真实标签文件路径\n",
    "    annotation_file = f'train_data/common_benchmarks/{dataset}/annotation.jsonl'\n",
    "\n",
    "    # 预测文件路径，假设你有四个预测文件，分别对应四种不同的方法\n",
    "    prediction_files = [\n",
    "        f'output/rec/rec_cloformer_cppd_base/rec_cloformer_cppd_base_{dataset}.txt',\n",
    "        f'output/rec/rec_svtr_base_none_ctc_en_train/predicts_svtr_base_{dataset}.txt',\n",
    "        f'output/rec/rec_r45_abinet_train/predicts_r45_abinet_{dataset}.txt',\n",
    "        f'output/rec/rec_r45_visionlan_train/predicts_r45_visionlan_{dataset}.txt'\n",
    "    ]\n",
    "\n",
    "    method_list = ['le_cp', 'svtr', 'abinet', 'vislan']\n",
    "\n",
    "    # 用于存储结果\n",
    "    output_file = f'output/rec/comparison_results_{dataset}.txt'\n",
    "\n",
    "    # 加载真实标签\n",
    "    true_labels = {}\n",
    "    with open(annotation_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            filename = data['filename'].split('/')[-1].lower()  # 为了大小写不敏感\n",
    "            true_labels[filename] = data['text'].lower()\n",
    "\n",
    "    # 函数用于加载预测结果\n",
    "    def load_predictions(file_path):\n",
    "        predictions = {}\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if not parts:\n",
    "                    continue\n",
    "                filename = parts[0].split('/')[-1].lower()\n",
    "                predicted_text = parts[1].lower()\n",
    "                predictions[filename] = predicted_text\n",
    "        return predictions\n",
    "\n",
    "    # 加载所有方法的预测结果\n",
    "    all_predictions = [load_predictions(pf) for pf in prediction_files]\n",
    "\n",
    "    # 比较并保存结果\n",
    "    with open(output_file, 'w', encoding='utf-8') as out_f:\n",
    "        for filename, true_text in true_labels.items():\n",
    "            # 检查所有方法对该文件的预测\n",
    "            predictions = [preds.get(filename, \"\").lower() for preds in all_predictions]\n",
    "            \n",
    "            # 方法1正确，其他至少一个错误\n",
    "            if predictions[0] == true_text and not all(pred == true_text for pred in predictions[1:]):\n",
    "                incorrect_methods = [f\"{method_list[i]}\" for i, pred in enumerate(predictions[1:], start=1) if pred != true_text]\n",
    "                print(f\"Filename: {filename}\")\n",
    "                out_f.write(f\"Filename: {filename}\\n\")\n",
    "                print(f\"True Label: \\t{true_text}\")\n",
    "                out_f.write(f\"True Label: \\t{true_text}\\n\")\n",
    "                for i, pred in enumerate(predictions, start=0):\n",
    "                    print(f\"Method {method_list[i]}: \\t{pred}\")\n",
    "                    out_f.write(f\"Method {method_list[i]}: \\t{pred}\\n\")\n",
    "                print(f\"Incorrect Methods: {', '.join(incorrect_methods)}\\n\")\n",
    "                out_f.write(f\"Incorrect Methods: {', '.join(incorrect_methods)}\\n\\n\")\n",
    "\n",
    "\n",
    "    # 请确保替换 'path_to_your_annotation_file.jsonl' 和 'prediction_method_X.txt'\n",
    "    # 为你的实际文件路径。\n",
    "\n",
    "\n",
    "\n",
    "def diff_all_wrong(dataset):\n",
    "    # 真实标签文件路径\n",
    "    annotation_file = f'train_data/common_benchmarks/{dataset}/annotation.jsonl'\n",
    "\n",
    "    # 预测文件路径，假设你有四个预测文件，分别对应四种不同的方法\n",
    "    prediction_files = [\n",
    "        f'output/rec/rec_cloformer_cppd_base/rec_cloformer_cppd_base_{dataset}.txt',\n",
    "        f'output/rec/rec_svtr_base_none_ctc_en_train/predicts_svtr_base_{dataset}.txt',\n",
    "        f'output/rec/rec_r45_abinet_train/predicts_r45_abinet_{dataset}.txt',\n",
    "        f'output/rec/rec_r45_visionlan_train/predicts_r45_visionlan_{dataset}.txt'\n",
    "    ]\n",
    "\n",
    "    method_list = ['le_cp', 'svtr', 'abinet', 'vislan']\n",
    "\n",
    "    # 用于存储结果\n",
    "    output_file = f'output/rec/all_wrong_comparison_results_{dataset}.txt'\n",
    "\n",
    "    # 加载真实标签\n",
    "    true_labels = {}\n",
    "    with open(annotation_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            filename = data['filename'].split('/')[-1].lower()  # 为了大小写不敏感\n",
    "            true_labels[filename] = data['text'].lower()\n",
    "\n",
    "    # 函数用于加载预测结果\n",
    "    def load_predictions(file_path):\n",
    "        predictions = {}\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if not parts:\n",
    "                    continue\n",
    "                filename = parts[0].split('/')[-1].lower()\n",
    "                predicted_text = parts[1].lower()\n",
    "                predictions[filename] = predicted_text\n",
    "        return predictions\n",
    "\n",
    "    # 加载所有方法的预测结果\n",
    "    all_predictions = [load_predictions(pf) for pf in prediction_files]\n",
    "\n",
    "    # 比较并保存结果\n",
    "    with open(output_file, 'w', encoding='utf-8') as out_f:\n",
    "        for filename, true_text in true_labels.items():\n",
    "            # 检查所有方法对该文件的预测\n",
    "            predictions = [preds.get(filename, \"\").lower() for preds in all_predictions]\n",
    "            \n",
    "            # 检查是否所有方法均预测错误\n",
    "            if all(pred != true_text for pred in predictions):\n",
    "                print(f\"Filename: {filename}\")\n",
    "                out_f.write(f\"Filename: {filename}\\n\")\n",
    "                print(f\"True Label: \\t{true_text}\")\n",
    "                out_f.write(f\"True Label: \\t{true_text}\\n\")\n",
    "                for i, pred in enumerate(predictions, start=0):\n",
    "                    print(f\"Method {method_list[i]}: \\t{pred}\")\n",
    "                    out_f.write(f\"Method {method_list[i]}: \\t{pred}\\n\")\n",
    "                print(\"\\n\")\n",
    "                out_f.write(\"\\n\\n\")\n",
    "                \n",
    "                \n",
    "\n",
    "def diff_special_case(dataset):\n",
    "    # 真实标签文件路径\n",
    "    annotation_file = f'train_data/common_benchmarks/{dataset}/annotation.jsonl'\n",
    "\n",
    "    # 预测文件路径，假设你有四个预测文件，分别对应四种不同的方法\n",
    "    prediction_files = [\n",
    "        f'output/rec/rec_cloformer_cppd_base/rec_cloformer_cppd_base_{dataset}.txt',\n",
    "        f'output/rec/rec_svtr_base_none_ctc_en_train/predicts_svtr_base_{dataset}.txt',\n",
    "        f'output/rec/rec_r45_abinet_train/predicts_r45_abinet_{dataset}.txt',\n",
    "        f'output/rec/rec_r45_visionlan_train/predicts_r45_visionlan_{dataset}.txt'\n",
    "    ]\n",
    "\n",
    "    method_list = ['le_cp', 'svtr', 'abinet', 'vislan']\n",
    "\n",
    "    # 用于存储结果的文件\n",
    "    output_file = f'output/rec/special_case_results_{dataset}.txt'\n",
    "\n",
    "    # 加载真实标签\n",
    "    true_labels = {}\n",
    "    with open(annotation_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            filename = data['filename'].split('/')[-1].lower()  # 为了大小写不敏感\n",
    "            true_labels[filename] = data['text'].lower()\n",
    "\n",
    "    # 函数用于加载预测结果\n",
    "    def load_predictions(file_path):\n",
    "        predictions = {}\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if not parts:\n",
    "                    continue\n",
    "                filename = parts[0].split('/')[-1].lower()\n",
    "                predicted_text = parts[1].lower()\n",
    "                predictions[filename] = predicted_text\n",
    "        return predictions\n",
    "\n",
    "    # 加载所有方法的预测结果\n",
    "    all_predictions = [load_predictions(pf) for pf in prediction_files]\n",
    "\n",
    "    # 比较并保存结果\n",
    "    with open(output_file, 'w', encoding='utf-8') as out_f:\n",
    "        for filename, true_text in true_labels.items():\n",
    "            # 检查所有方法对该文件的预测\n",
    "            predictions = [preds.get(filename, \"\").lower() for preds in all_predictions]\n",
    "            \n",
    "            # le_cp预测错误，其他任一方法成功预测\n",
    "            if predictions[0] != true_text and any(pred == true_text for pred in predictions[1:]):\n",
    "                successful_methods = [f\"{method_list[i]}\" for i, pred in enumerate(predictions[1:], start=1) if pred == true_text]\n",
    "                print(f\"Filename: {filename}\")\n",
    "                out_f.write(f\"Filename: {filename}\\n\")\n",
    "                print(f\"True Label: \\t{true_text}\")\n",
    "                out_f.write(f\"True Label: \\t{true_text}\\n\")\n",
    "                for i, pred in enumerate(predictions, start=0):\n",
    "                    print(f\"Method {method_list[i]}: \\t{pred}\")\n",
    "                    out_f.write(f\"Method {method_list[i]}: \\t{pred}\\n\")\n",
    "                print(f\"Successful Methods: {', '.join(successful_methods)}\\n\")\n",
    "                out_f.write(f\"Successful Methods: {', '.join(successful_methods)}\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=['IC13', 'SVT', 'IIIT5K', 'IC15', 'SVTP', 'CUTE80']\n",
    "\n",
    "for dataset in datasets:\n",
    "    diff_all(dataset)\n",
    "    print('compare compelet!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=['IC13', 'SVT', 'IIIT5K', 'IC15', 'SVTP', 'CUTE80']\n",
    "\n",
    "for dataset in datasets:\n",
    "    diff_all_wrong(dataset)\n",
    "    print('compare compelet!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=['IC13', 'SVT', 'IIIT5K', 'IC15', 'SVTP', 'CUTE80']\n",
    "\n",
    "for dataset in datasets:\n",
    "    diff_special_case(dataset)\n",
    "    print('compare compelet!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diff ours method on u14m benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def diff_all(dataset):\n",
    "    # 真实标签文件路径\n",
    "    annotation_file = f'/root/autodl-tmp/Union14M-L/Union14M-Benchmarks/{dataset}/annotation.jsonl'\n",
    "\n",
    "    # 预测文件路径，假设你有四个预测文件，分别对应四种不同的方法\n",
    "    prediction_files = [\n",
    "        f'output/rec/rec_cloformer_cppd_base/predicts_cloformer_cppd_base_{dataset}.txt',\n",
    "        f'output/rec/rec_cloformer_cppd_small/predicts_cloformer_cppd_small_{dataset}.txt',\n",
    "        f'output/rec/rec_cloformer_cppd_tiny/predicts_cloformer_cppd_tiny_{dataset}.txt',\n",
    "\n",
    "    ]\n",
    "\n",
    "    method_list = ['base', 'small', 'tiny', ]\n",
    "\n",
    "    # 用于存储结果\n",
    "    output_file = f'output/rec/comparison_results_{dataset}.txt'\n",
    "\n",
    "    # 加载真实标签\n",
    "    true_labels = {}\n",
    "    with open(annotation_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            filename = data['filename'].split('/')[-1].lower()  # 为了大小写不敏感\n",
    "            true_labels[filename] = data['text'].lower()\n",
    "\n",
    "    # 函数用于加载预测结果\n",
    "    def load_predictions(file_path):\n",
    "        predictions = {}\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if not parts:\n",
    "                    continue\n",
    "                filename = parts[0].split('/')[-1].lower()\n",
    "                predicted_text = parts[1].lower()\n",
    "                predictions[filename] = predicted_text\n",
    "        return predictions\n",
    "\n",
    "    # 加载所有方法的预测结果\n",
    "    all_predictions = [load_predictions(pf) for pf in prediction_files]\n",
    "\n",
    "    # 比较并保存结果\n",
    "    with open(output_file, 'w', encoding='utf-8') as out_f:\n",
    "        for filename, true_text in true_labels.items():\n",
    "            # 检查所有方法对该文件的预测\n",
    "            predictions = [preds.get(filename, \"\").lower() for preds in all_predictions]\n",
    "            \n",
    "            # 方法1正确，其他至少一个错误\n",
    "            if predictions[0] == true_text and not all(pred == true_text for pred in predictions[1:]):\n",
    "                incorrect_methods = [f\"{method_list[i]}\" for i, pred in enumerate(predictions[1:], start=1) if pred != true_text]\n",
    "                print(f\"Filename: {filename}\")\n",
    "                out_f.write(f\"Filename: {filename}\\n\")\n",
    "                print(f\"True Label: \\t{true_text}\")\n",
    "                out_f.write(f\"True Label: \\t{true_text}\\n\")\n",
    "                for i, pred in enumerate(predictions, start=0):\n",
    "                    print(f\"Method {method_list[i]}: \\t{pred}\")\n",
    "                    out_f.write(f\"Method {method_list[i]}: \\t{pred}\\n\")\n",
    "                print(f\"Incorrect Methods: {', '.join(incorrect_methods)}\\n\")\n",
    "                out_f.write(f\"Incorrect Methods: {', '.join(incorrect_methods)}\\n\\n\")\n",
    "\n",
    "\n",
    "    # 请确保替换 'path_to_your_annotation_file.jsonl' 和 'prediction_method_X.txt'\n",
    "    # 为你的实际文件路径。\n",
    "\n",
    "\n",
    "\n",
    "def diff_all_wrong(dataset):\n",
    "    # 真实标签文件路径\n",
    "    annotation_file = f'train_data/common_benchmarks/{dataset}/annotation.jsonl'\n",
    "\n",
    "    # 预测文件路径，假设你有四个预测文件，分别对应四种不同的方法\n",
    "    prediction_files = [\n",
    "        f'output/rec/rec_cloformer_cppd_base/rec_cloformer_cppd_base_{dataset}.txt',\n",
    "        f'output/rec/rec_svtr_base_none_ctc_en_train/predicts_svtr_base_{dataset}.txt',\n",
    "        f'output/rec/rec_r45_abinet_train/predicts_r45_abinet_{dataset}.txt',\n",
    "        f'output/rec/rec_r45_visionlan_train/predicts_r45_visionlan_{dataset}.txt'\n",
    "    ]\n",
    "\n",
    "    method_list = ['le_cp', 'svtr', 'abinet', 'vislan']\n",
    "\n",
    "    # 用于存储结果\n",
    "    output_file = f'output/rec/all_wrong_comparison_results_{dataset}.txt'\n",
    "\n",
    "    # 加载真实标签\n",
    "    true_labels = {}\n",
    "    with open(annotation_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            filename = data['filename'].split('/')[-1].lower()  # 为了大小写不敏感\n",
    "            true_labels[filename] = data['text'].lower()\n",
    "\n",
    "    # 函数用于加载预测结果\n",
    "    def load_predictions(file_path):\n",
    "        predictions = {}\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if not parts:\n",
    "                    continue\n",
    "                filename = parts[0].split('/')[-1].lower()\n",
    "                predicted_text = parts[1].lower()\n",
    "                predictions[filename] = predicted_text\n",
    "        return predictions\n",
    "\n",
    "    # 加载所有方法的预测结果\n",
    "    all_predictions = [load_predictions(pf) for pf in prediction_files]\n",
    "\n",
    "    # 比较并保存结果\n",
    "    with open(output_file, 'w', encoding='utf-8') as out_f:\n",
    "        for filename, true_text in true_labels.items():\n",
    "            # 检查所有方法对该文件的预测\n",
    "            predictions = [preds.get(filename, \"\").lower() for preds in all_predictions]\n",
    "            \n",
    "            # 检查是否所有方法均预测错误\n",
    "            if all(pred != true_text for pred in predictions):\n",
    "                print(f\"Filename: {filename}\")\n",
    "                out_f.write(f\"Filename: {filename}\\n\")\n",
    "                print(f\"True Label: \\t{true_text}\")\n",
    "                out_f.write(f\"True Label: \\t{true_text}\\n\")\n",
    "                for i, pred in enumerate(predictions, start=0):\n",
    "                    print(f\"Method {method_list[i]}: \\t{pred}\")\n",
    "                    out_f.write(f\"Method {method_list[i]}: \\t{pred}\\n\")\n",
    "                print(\"\\n\")\n",
    "                out_f.write(\"\\n\\n\")\n",
    "                \n",
    "                \n",
    "\n",
    "def diff_special_case(dataset):\n",
    "    # 真实标签文件路径\n",
    "    annotation_file = f'train_data/common_benchmarks/{dataset}/annotation.jsonl'\n",
    "\n",
    "    # 预测文件路径，假设你有四个预测文件，分别对应四种不同的方法\n",
    "    prediction_files = [\n",
    "        f'output/rec/rec_cloformer_cppd_base/rec_cloformer_cppd_base_{dataset}.txt',\n",
    "        f'output/rec/rec_svtr_base_none_ctc_en_train/predicts_svtr_base_{dataset}.txt',\n",
    "        f'output/rec/rec_r45_abinet_train/predicts_r45_abinet_{dataset}.txt',\n",
    "        f'output/rec/rec_r45_visionlan_train/predicts_r45_visionlan_{dataset}.txt'\n",
    "    ]\n",
    "\n",
    "    method_list = ['le_cp', 'svtr', 'abinet', 'vislan']\n",
    "\n",
    "    # 用于存储结果的文件\n",
    "    output_file = f'output/rec/special_case_results_{dataset}.txt'\n",
    "\n",
    "    # 加载真实标签\n",
    "    true_labels = {}\n",
    "    with open(annotation_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            filename = data['filename'].split('/')[-1].lower()  # 为了大小写不敏感\n",
    "            true_labels[filename] = data['text'].lower()\n",
    "\n",
    "    # 函数用于加载预测结果\n",
    "    def load_predictions(file_path):\n",
    "        predictions = {}\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if not parts:\n",
    "                    continue\n",
    "                filename = parts[0].split('/')[-1].lower()\n",
    "                predicted_text = parts[1].lower()\n",
    "                predictions[filename] = predicted_text\n",
    "        return predictions\n",
    "\n",
    "    # 加载所有方法的预测结果\n",
    "    all_predictions = [load_predictions(pf) for pf in prediction_files]\n",
    "\n",
    "    # 比较并保存结果\n",
    "    with open(output_file, 'w', encoding='utf-8') as out_f:\n",
    "        for filename, true_text in true_labels.items():\n",
    "            # 检查所有方法对该文件的预测\n",
    "            predictions = [preds.get(filename, \"\").lower() for preds in all_predictions]\n",
    "            \n",
    "            # le_cp预测错误，其他任一方法成功预测\n",
    "            if predictions[0] != true_text and any(pred == true_text for pred in predictions[1:]):\n",
    "                successful_methods = [f\"{method_list[i]}\" for i, pred in enumerate(predictions[1:], start=1) if pred == true_text]\n",
    "                print(f\"Filename: {filename}\")\n",
    "                out_f.write(f\"Filename: {filename}\\n\")\n",
    "                print(f\"True Label: \\t{true_text}\")\n",
    "                out_f.write(f\"True Label: \\t{true_text}\\n\")\n",
    "                for i, pred in enumerate(predictions, start=0):\n",
    "                    print(f\"Method {method_list[i]}: \\t{pred}\")\n",
    "                    out_f.write(f\"Method {method_list[i]}: \\t{pred}\\n\")\n",
    "                print(f\"Successful Methods: {', '.join(successful_methods)}\\n\")\n",
    "                out_f.write(f\"Successful Methods: {', '.join(successful_methods)}\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=['artistic', 'contextless', 'curve', 'multi_oriented', 'multi_words', 'salient']\n",
    "\n",
    "for dataset in datasets:\n",
    "    diff_all(dataset)\n",
    "    print('compare compelet!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=['general']\n",
    "\n",
    "for dataset in datasets:\n",
    "    diff_all(dataset)\n",
    "    print('compare compelet!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
