{
    "sourceFile": "ppocr/modeling/backbones/rec_svtrnet.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1705907772566,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1705907880688,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -587,10 +587,10 @@\n         if self.use_lenhead:\r\n             return x, len_x\r\n         return x\r\n \r\n-\r\n-if __name__ == \"__mian__\":\r\n+if __name__ == \"__main__\":\r\n+    print('fff')\r\n     svtr = SVTRNet()\r\n     input = paddle.randn([1, 3, 32, 100])\r\n     out = svtr(input)\r\n     print(out.shape)\n\\ No newline at end of file\n"
                }
            ],
            "date": 1705907772566,
            "name": "Commit-0",
            "content": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#    http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\nfrom paddle import ParamAttr\r\nfrom paddle.nn.initializer import KaimingNormal\r\nimport numpy as np\r\nimport paddle\r\nimport paddle.nn as nn\r\nfrom paddle.nn.initializer import TruncatedNormal, Constant, Normal\r\n\r\ntrunc_normal_ = TruncatedNormal(std=.02)\r\nnormal_ = Normal\r\nzeros_ = Constant(value=0.)\r\nones_ = Constant(value=1.)\r\n\r\n\r\ndef drop_path(x, drop_prob=0., training=False):\r\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\r\n    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\r\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ...\r\n    \"\"\"\r\n    if drop_prob == 0. or not training:\r\n        return x\r\n    keep_prob = paddle.to_tensor(1 - drop_prob, dtype=x.dtype)\r\n    shape = (paddle.shape(x)[0], ) + (1, ) * (x.ndim - 1)\r\n    random_tensor = keep_prob + paddle.rand(shape, dtype=x.dtype)\r\n    random_tensor = paddle.floor(random_tensor)  # binarize\r\n    output = x.divide(keep_prob) * random_tensor\r\n    return output\r\n\r\n\r\nclass ConvBNLayer(nn.Layer):\r\n    def __init__(self,\r\n                 in_channels,\r\n                 out_channels,\r\n                 kernel_size=3,\r\n                 stride=1,\r\n                 padding=0,\r\n                 bias_attr=False,\r\n                 groups=1,\r\n                 act=nn.GELU):\r\n        super().__init__()\r\n        self.conv = nn.Conv2D(\r\n            in_channels=in_channels,\r\n            out_channels=out_channels,\r\n            kernel_size=kernel_size,\r\n            stride=stride,\r\n            padding=padding,\r\n            groups=groups,\r\n            weight_attr=paddle.ParamAttr(\r\n                initializer=nn.initializer.KaimingUniform()),\r\n            bias_attr=bias_attr)\r\n        self.norm = nn.BatchNorm2D(out_channels)\r\n        self.act = act()\r\n\r\n    def forward(self, inputs):\r\n        out = self.conv(inputs)\r\n        out = self.norm(out)\r\n        out = self.act(out)\r\n        return out\r\n\r\n\r\nclass DropPath(nn.Layer):\r\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\r\n    \"\"\"\r\n\r\n    def __init__(self, drop_prob=None):\r\n        super(DropPath, self).__init__()\r\n        self.drop_prob = drop_prob\r\n\r\n    def forward(self, x):\r\n        return drop_path(x, self.drop_prob, self.training)\r\n\r\n\r\nclass Identity(nn.Layer):\r\n    def __init__(self):\r\n        super(Identity, self).__init__()\r\n\r\n    def forward(self, input):\r\n        return input\r\n\r\n\r\nclass Mlp(nn.Layer):\r\n    def __init__(self,\r\n                 in_features,\r\n                 hidden_features=None,\r\n                 out_features=None,\r\n                 act_layer=nn.GELU,\r\n                 drop=0.):\r\n        super().__init__()\r\n        out_features = out_features or in_features\r\n        hidden_features = hidden_features or in_features\r\n        self.fc1 = nn.Linear(in_features, hidden_features)\r\n        self.act = act_layer()\r\n        self.fc2 = nn.Linear(hidden_features, out_features)\r\n        self.drop = nn.Dropout(drop)\r\n\r\n    def forward(self, x):\r\n        x = self.fc1(x)\r\n        x = self.act(x)\r\n        x = self.drop(x)\r\n        x = self.fc2(x)\r\n        x = self.drop(x)\r\n        return x\r\n\r\n\r\nclass ConvMixer(nn.Layer):\r\n    def __init__(\r\n            self,\r\n            dim,\r\n            num_heads=8,\r\n            HW=[8, 25],\r\n            local_k=[3, 3], ):\r\n        super().__init__()\r\n        self.HW = HW\r\n        self.dim = dim\r\n        self.local_mixer = nn.Conv2D(\r\n            dim,\r\n            dim,\r\n            local_k,\r\n            1, [local_k[0] // 2, local_k[1] // 2],\r\n            groups=num_heads,\r\n            weight_attr=ParamAttr(initializer=KaimingNormal()))\r\n\r\n    def forward(self, x):\r\n        h = self.HW[0]\r\n        w = self.HW[1]\r\n        x = x.transpose([0, 2, 1]).reshape([0, self.dim, h, w])\r\n        x = self.local_mixer(x)\r\n        x = x.flatten(2).transpose([0, 2, 1])\r\n        return x\r\n\r\n\r\nclass Attention(nn.Layer):\r\n    def __init__(self,\r\n                 dim,\r\n                 num_heads=8,\r\n                 mixer='Global',\r\n                 HW=None,\r\n                 local_k=[7, 11],\r\n                 qkv_bias=False,\r\n                 qk_scale=None,\r\n                 attn_drop=0.,\r\n                 proj_drop=0.):\r\n        super().__init__()\r\n        self.num_heads = num_heads\r\n        self.dim = dim\r\n        self.head_dim = dim // num_heads\r\n        self.scale = qk_scale or self.head_dim**-0.5\r\n\r\n        self.qkv = nn.Linear(dim, dim * 3, bias_attr=qkv_bias)\r\n        self.attn_drop = nn.Dropout(attn_drop)\r\n        self.proj = nn.Linear(dim, dim)\r\n        self.proj_drop = nn.Dropout(proj_drop)\r\n        self.HW = HW\r\n        if HW is not None:\r\n            H = HW[0]\r\n            W = HW[1]\r\n            self.N = H * W\r\n            self.C = dim\r\n        if mixer == 'Local' and HW is not None:\r\n            hk = local_k[0]\r\n            wk = local_k[1]\r\n            mask = paddle.ones([H * W, H + hk - 1, W + wk - 1], dtype='float32')\r\n            for h in range(0, H):\r\n                for w in range(0, W):\r\n                    mask[h * W + w, h:h + hk, w:w + wk] = 0.\r\n            mask_paddle = mask[:, hk // 2:H + hk // 2, wk // 2:W + wk //\r\n                               2].flatten(1)\r\n            mask_inf = paddle.full([H * W, H * W], '-inf', dtype='float32')\r\n            mask = paddle.where(mask_paddle < 1, mask_paddle, mask_inf)\r\n            self.mask = mask.unsqueeze([0, 1])\r\n        self.mixer = mixer\r\n\r\n    def forward(self, x):\r\n        qkv = self.qkv(x).reshape(\r\n            (0, -1, 3, self.num_heads, self.head_dim)).transpose(\r\n                (2, 0, 3, 1, 4))\r\n        q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\r\n\r\n        attn = (q.matmul(k.transpose((0, 1, 3, 2))))\r\n        if self.mixer == 'Local':\r\n            attn += self.mask\r\n        attn = nn.functional.softmax(attn, axis=-1)\r\n        attn = self.attn_drop(attn)\r\n\r\n        x = (attn.matmul(v)).transpose((0, 2, 1, 3)).reshape((0, -1, self.dim))\r\n        x = self.proj(x)\r\n        x = self.proj_drop(x)\r\n        return x\r\n\r\n\r\nclass Block(nn.Layer):\r\n    def __init__(self,\r\n                 dim,\r\n                 num_heads,\r\n                 mixer='Global',\r\n                 local_mixer=[7, 11],\r\n                 HW=None,\r\n                 mlp_ratio=4.,\r\n                 qkv_bias=False,\r\n                 qk_scale=None,\r\n                 drop=0.,\r\n                 attn_drop=0.,\r\n                 drop_path=0.,\r\n                 act_layer=nn.GELU,\r\n                 norm_layer='nn.LayerNorm',\r\n                 epsilon=1e-6,\r\n                 prenorm=True):\r\n        super().__init__()\r\n        if isinstance(norm_layer, str):\r\n            self.norm1 = eval(norm_layer)(dim, epsilon=epsilon)\r\n        else:\r\n            self.norm1 = norm_layer(dim)\r\n        if mixer == 'Global' or mixer == 'Local':\r\n            self.mixer = Attention(\r\n                dim,\r\n                num_heads=num_heads,\r\n                mixer=mixer,\r\n                HW=HW,\r\n                local_k=local_mixer,\r\n                qkv_bias=qkv_bias,\r\n                qk_scale=qk_scale,\r\n                attn_drop=attn_drop,\r\n                proj_drop=drop)\r\n        elif mixer == 'Conv':\r\n            self.mixer = ConvMixer(\r\n                dim, num_heads=num_heads, HW=HW, local_k=local_mixer)\r\n        else:\r\n            raise TypeError(\"The mixer must be one of [Global, Local, Conv]\")\r\n\r\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else Identity()\r\n        if isinstance(norm_layer, str):\r\n            self.norm2 = eval(norm_layer)(dim, epsilon=epsilon)\r\n        else:\r\n            self.norm2 = norm_layer(dim)\r\n        mlp_hidden_dim = int(dim * mlp_ratio)\r\n        self.mlp_ratio = mlp_ratio\r\n        self.mlp = Mlp(in_features=dim,\r\n                       hidden_features=mlp_hidden_dim,\r\n                       act_layer=act_layer,\r\n                       drop=drop)\r\n        self.prenorm = prenorm\r\n\r\n    def forward(self, x):\r\n        if self.prenorm:\r\n            x = self.norm1(x + self.drop_path(self.mixer(x)))\r\n            x = self.norm2(x + self.drop_path(self.mlp(x)))\r\n        else:\r\n            x = x + self.drop_path(self.mixer(self.norm1(x)))\r\n            x = x + self.drop_path(self.mlp(self.norm2(x)))\r\n        return x\r\n\r\n\r\nclass PatchEmbed(nn.Layer):\r\n    \"\"\" Image to Patch Embedding\r\n    \"\"\"\r\n\r\n    def __init__(self,\r\n                 img_size=[32, 100],\r\n                 in_channels=3,\r\n                 embed_dim=768,\r\n                 sub_num=2,\r\n                 patch_size=[4, 4],\r\n                 mode='pope'):\r\n        super().__init__()\r\n        num_patches = (img_size[1] // (2 ** sub_num)) * \\\r\n                      (img_size[0] // (2 ** sub_num))\r\n        self.img_size = img_size\r\n        self.num_patches = num_patches\r\n        self.embed_dim = embed_dim\r\n        self.norm = None\r\n        if mode == 'pope':\r\n            if sub_num == 2:\r\n                self.proj = nn.Sequential(\r\n                    ConvBNLayer(\r\n                        in_channels=in_channels,\r\n                        out_channels=embed_dim // 2,\r\n                        kernel_size=3,\r\n                        stride=2,\r\n                        padding=1,\r\n                        act=nn.GELU,\r\n                        bias_attr=None),\r\n                    ConvBNLayer(\r\n                        in_channels=embed_dim // 2,\r\n                        out_channels=embed_dim,\r\n                        kernel_size=3,\r\n                        stride=2,\r\n                        padding=1,\r\n                        act=nn.GELU,\r\n                        bias_attr=None))\r\n            if sub_num == 3:\r\n                self.proj = nn.Sequential(\r\n                    ConvBNLayer(\r\n                        in_channels=in_channels,\r\n                        out_channels=embed_dim // 4,\r\n                        kernel_size=3,\r\n                        stride=2,\r\n                        padding=1,\r\n                        act=nn.GELU,\r\n                        bias_attr=None),\r\n                    ConvBNLayer(\r\n                        in_channels=embed_dim // 4,\r\n                        out_channels=embed_dim // 2,\r\n                        kernel_size=3,\r\n                        stride=2,\r\n                        padding=1,\r\n                        act=nn.GELU,\r\n                        bias_attr=None),\r\n                    ConvBNLayer(\r\n                        in_channels=embed_dim // 2,\r\n                        out_channels=embed_dim,\r\n                        kernel_size=3,\r\n                        stride=2,\r\n                        padding=1,\r\n                        act=nn.GELU,\r\n                        bias_attr=None))\r\n        elif mode == 'linear':\r\n            self.proj = nn.Conv2D(\r\n                1, embed_dim, kernel_size=patch_size, stride=patch_size)\r\n            self.num_patches = img_size[0] // patch_size[0] * img_size[\r\n                1] // patch_size[1]\r\n\r\n    def forward(self, x):\r\n        B, C, H, W = x.shape\r\n        assert H == self.img_size[0] and W == self.img_size[1], \\\r\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\r\n        x = self.proj(x).flatten(2).transpose((0, 2, 1))\r\n        return x\r\n\r\n\r\nclass SubSample(nn.Layer):\r\n    def __init__(self,\r\n                 in_channels,\r\n                 out_channels,\r\n                 types='Pool',\r\n                 stride=[2, 1],\r\n                 sub_norm='nn.LayerNorm',\r\n                 act=None):\r\n        super().__init__()\r\n        self.types = types\r\n        if types == 'Pool':\r\n            self.avgpool = nn.AvgPool2D(\r\n                kernel_size=[3, 5], stride=stride, padding=[1, 2])\r\n            self.maxpool = nn.MaxPool2D(\r\n                kernel_size=[3, 5], stride=stride, padding=[1, 2])\r\n            self.proj = nn.Linear(in_channels, out_channels)\r\n        else:\r\n            self.conv = nn.Conv2D(\r\n                in_channels,\r\n                out_channels,\r\n                kernel_size=3,\r\n                stride=stride,\r\n                padding=1,\r\n                weight_attr=ParamAttr(initializer=KaimingNormal()))\r\n        self.norm = eval(sub_norm)(out_channels)\r\n        if act is not None:\r\n            self.act = act()\r\n        else:\r\n            self.act = None\r\n\r\n    def forward(self, x):\r\n\r\n        if self.types == 'Pool':\r\n            x1 = self.avgpool(x)\r\n            x2 = self.maxpool(x)\r\n            x = (x1 + x2) * 0.5\r\n            out = self.proj(x.flatten(2).transpose((0, 2, 1)))\r\n        else:\r\n            x = self.conv(x)\r\n            out = x.flatten(2).transpose((0, 2, 1))\r\n        out = self.norm(out)\r\n        if self.act is not None:\r\n            out = self.act(out)\r\n\r\n        return out\r\n\r\n\r\nclass SVTRNet(nn.Layer):\r\n    def __init__(\r\n            self,\r\n            img_size=[32, 100],\r\n            in_channels=3,\r\n            embed_dim=[64, 128, 256],\r\n            depth=[3, 6, 3],\r\n            num_heads=[2, 4, 8],\r\n            mixer=['Local'] * 6 + ['Global'] *\r\n            6,  # Local atten, Global atten, Conv\r\n            local_mixer=[[7, 11], [7, 11], [7, 11]],\r\n            patch_merging='Conv',  # Conv, Pool, None\r\n            mlp_ratio=4,\r\n            qkv_bias=True,\r\n            qk_scale=None,\r\n            drop_rate=0.,\r\n            last_drop=0.1,\r\n            attn_drop_rate=0.,\r\n            drop_path_rate=0.1,\r\n            norm_layer='nn.LayerNorm',\r\n            sub_norm='nn.LayerNorm',\r\n            epsilon=1e-6,\r\n            out_channels=192,\r\n            out_char_num=25,\r\n            block_unit='Block',\r\n            act='nn.GELU',\r\n            last_stage=True,\r\n            sub_num=2,\r\n            prenorm=True,\r\n            use_lenhead=False,\r\n            **kwargs):\r\n        super().__init__()\r\n        self.img_size = img_size\r\n        self.embed_dim = embed_dim\r\n        self.out_channels = out_channels\r\n        self.prenorm = prenorm\r\n        patch_merging = None if patch_merging != 'Conv' and patch_merging != 'Pool' else patch_merging\r\n        self.patch_embed = PatchEmbed(\r\n            img_size=img_size,\r\n            in_channels=in_channels,\r\n            embed_dim=embed_dim[0],\r\n            sub_num=sub_num)\r\n        num_patches = self.patch_embed.num_patches\r\n        self.HW = [img_size[0] // (2**sub_num), img_size[1] // (2**sub_num)]\r\n        self.pos_embed = self.create_parameter(\r\n            shape=[1, num_patches, embed_dim[0]], default_initializer=zeros_)\r\n        self.add_parameter(\"pos_embed\", self.pos_embed)\r\n        self.pos_drop = nn.Dropout(p=drop_rate)\r\n        Block_unit = eval(block_unit)\r\n\r\n        dpr = np.linspace(0, drop_path_rate, sum(depth))\r\n        self.blocks1 = nn.LayerList([\r\n            Block_unit(\r\n                dim=embed_dim[0],\r\n                num_heads=num_heads[0],\r\n                mixer=mixer[0:depth[0]][i],\r\n                HW=self.HW,\r\n                local_mixer=local_mixer[0],\r\n                mlp_ratio=mlp_ratio,\r\n                qkv_bias=qkv_bias,\r\n                qk_scale=qk_scale,\r\n                drop=drop_rate,\r\n                act_layer=eval(act),\r\n                attn_drop=attn_drop_rate,\r\n                drop_path=dpr[0:depth[0]][i],\r\n                norm_layer=norm_layer,\r\n                epsilon=epsilon,\r\n                prenorm=prenorm) for i in range(depth[0])\r\n        ])\r\n        if patch_merging is not None:\r\n            self.sub_sample1 = SubSample(\r\n                embed_dim[0],\r\n                embed_dim[1],\r\n                sub_norm=sub_norm,\r\n                stride=[2, 1],\r\n                types=patch_merging)\r\n            HW = [self.HW[0] // 2, self.HW[1]]\r\n        else:\r\n            HW = self.HW\r\n        self.patch_merging = patch_merging\r\n        self.blocks2 = nn.LayerList([\r\n            Block_unit(\r\n                dim=embed_dim[1],\r\n                num_heads=num_heads[1],\r\n                mixer=mixer[depth[0]:depth[0] + depth[1]][i],\r\n                HW=HW,\r\n                local_mixer=local_mixer[1],\r\n                mlp_ratio=mlp_ratio,\r\n                qkv_bias=qkv_bias,\r\n                qk_scale=qk_scale,\r\n                drop=drop_rate,\r\n                act_layer=eval(act),\r\n                attn_drop=attn_drop_rate,\r\n                drop_path=dpr[depth[0]:depth[0] + depth[1]][i],\r\n                norm_layer=norm_layer,\r\n                epsilon=epsilon,\r\n                prenorm=prenorm) for i in range(depth[1])\r\n        ])\r\n        if patch_merging is not None:\r\n            self.sub_sample2 = SubSample(\r\n                embed_dim[1],\r\n                embed_dim[2],\r\n                sub_norm=sub_norm,\r\n                stride=[2, 1],\r\n                types=patch_merging)\r\n            HW = [self.HW[0] // 4, self.HW[1]]\r\n        else:\r\n            HW = self.HW\r\n        self.blocks3 = nn.LayerList([\r\n            Block_unit(\r\n                dim=embed_dim[2],\r\n                num_heads=num_heads[2],\r\n                mixer=mixer[depth[0] + depth[1]:][i],\r\n                HW=HW,\r\n                local_mixer=local_mixer[2],\r\n                mlp_ratio=mlp_ratio,\r\n                qkv_bias=qkv_bias,\r\n                qk_scale=qk_scale,\r\n                drop=drop_rate,\r\n                act_layer=eval(act),\r\n                attn_drop=attn_drop_rate,\r\n                drop_path=dpr[depth[0] + depth[1]:][i],\r\n                norm_layer=norm_layer,\r\n                epsilon=epsilon,\r\n                prenorm=prenorm) for i in range(depth[2])\r\n        ])\r\n        self.last_stage = last_stage\r\n        if last_stage:\r\n            self.avg_pool = nn.AdaptiveAvgPool2D([1, out_char_num])\r\n            self.last_conv = nn.Conv2D(\r\n                in_channels=embed_dim[2],\r\n                out_channels=self.out_channels,\r\n                kernel_size=1,\r\n                stride=1,\r\n                padding=0,\r\n                bias_attr=False)\r\n            self.hardswish = nn.Hardswish()\r\n            self.dropout = nn.Dropout(p=last_drop, mode=\"downscale_in_infer\")\r\n        if not prenorm:\r\n            self.norm = eval(norm_layer)(embed_dim[-1], epsilon=epsilon)\r\n        self.use_lenhead = use_lenhead\r\n        if use_lenhead:\r\n            self.len_conv = nn.Linear(embed_dim[2], self.out_channels)\r\n            self.hardswish_len = nn.Hardswish()\r\n            self.dropout_len = nn.Dropout(\r\n                p=last_drop, mode=\"downscale_in_infer\")\r\n\r\n        trunc_normal_(self.pos_embed)\r\n        self.apply(self._init_weights)\r\n\r\n    def _init_weights(self, m):\r\n        if isinstance(m, nn.Linear):\r\n            trunc_normal_(m.weight)\r\n            if isinstance(m, nn.Linear) and m.bias is not None:\r\n                zeros_(m.bias)\r\n        elif isinstance(m, nn.LayerNorm):\r\n            zeros_(m.bias)\r\n            ones_(m.weight)\r\n\r\n    def forward_features(self, x):\r\n        x = self.patch_embed(x)\r\n        x = x + self.pos_embed\r\n        x = self.pos_drop(x)\r\n        for blk in self.blocks1:\r\n            x = blk(x)\r\n        if self.patch_merging is not None:\r\n            x = self.sub_sample1(\r\n                x.transpose([0, 2, 1]).reshape(\r\n                    [0, self.embed_dim[0], self.HW[0], self.HW[1]]))\r\n        for blk in self.blocks2:\r\n            x = blk(x)\r\n        if self.patch_merging is not None:\r\n            x = self.sub_sample2(\r\n                x.transpose([0, 2, 1]).reshape(\r\n                    [0, self.embed_dim[1], self.HW[0] // 2, self.HW[1]]))\r\n        for blk in self.blocks3:\r\n            x = blk(x)\r\n        if not self.prenorm:\r\n            x = self.norm(x)\r\n        return x\r\n\r\n    def forward(self, x):\r\n        x = self.forward_features(x)\r\n        if self.use_lenhead:\r\n            len_x = self.len_conv(x.mean(1))\r\n            len_x = self.dropout_len(self.hardswish_len(len_x))\r\n        if self.last_stage:\r\n            if self.patch_merging is not None:\r\n                h = self.HW[0] // 4\r\n            else:\r\n                h = self.HW[0]\r\n            x = self.avg_pool(\r\n                x.transpose([0, 2, 1]).reshape(\r\n                    [0, self.embed_dim[2], h, self.HW[1]]))\r\n            x = self.last_conv(x)\r\n            x = self.hardswish(x)\r\n            x = self.dropout(x)\r\n        if self.use_lenhead:\r\n            return x, len_x\r\n        return x\r\n\r\n\r\nif __name__ == \"__mian__\":\r\n    svtr = SVTRNet()\r\n    input = paddle.randn([1, 3, 32, 100])\r\n    out = svtr(input)\r\n    print(out.shape)"
        }
    ]
}