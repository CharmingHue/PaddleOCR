{
    "sourceFile": "ppocr/modeling/backbones/clolayer.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 24,
            "patches": [
                {
                    "date": 1709197351357,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1709198130887,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,5 +1,4 @@\n-from timm.models.layers import DropPath\r\n from typing import List\r\n \r\n import math\r\n from functools import partial\r\n@@ -26,21 +25,8 @@\n     def forward(self, x):\r\n         return SwishImplementation.apply(x)\r\n \r\n \r\n-\r\n-class AttnMap(nn.Module):\r\n-    def __init__(self, dim):\r\n-        super().__init__()\r\n-        self.act_block = nn.Sequential(\r\n-                            nn.Conv2d(dim, dim, 1, 1, 0),\r\n-                            MemoryEfficientSwish(),\r\n-                            nn.Conv2d(dim, dim, 1, 1, 0)\r\n-                            #nn.Identity()\r\n-                         )\r\n-    def forward(self, x):\r\n-        return self.act_block(x)\r\n-    \r\n def drop_path(x, drop_prob=0., training=False):\r\n     \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\r\n     the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\r\n     See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ...\r\n@@ -67,4 +53,163 @@\n         return drop_path(x, self.drop_prob, self.training)\r\n     \r\n     def extra_repr(self):\r\n         return f'drop_prob={round(self.drop_prob,3):0.3f}'\r\n+\r\n+\r\n+\r\n+class AttnMap(nn.Layer):\r\n+    def __init__(self, dim):\r\n+        super().__init__()\r\n+        self.act_block = nn.Sequential(\r\n+                            nn.Conv2D(dim, dim, 1, 1, 0),\r\n+                            MemoryEfficientSwish(),\r\n+                            nn.Conv2D(dim, dim, 1, 1, 0)\r\n+                            #nn.Identity()\r\n+                         )\r\n+    def forward(self, x):\r\n+        return self.act_block(x)\r\n+    \r\n+class EfficientAttention(nn.Layer):\r\n+\r\n+    def __init__(self, dim, num_heads, group_split: List[int], kernel_sizes: List[int], window_size=7, \r\n+                 attn_drop=0., proj_drop=0., qkv_bias=True):\r\n+        super().__init__()\r\n+        assert sum(group_split) == num_heads\r\n+        assert len(kernel_sizes) + 1 == len(group_split)\r\n+        self.dim = dim\r\n+        self.num_heads = num_heads\r\n+        self.dim_head = dim // num_heads\r\n+        self.scalor = self.dim_head ** -0.5\r\n+        self.kernel_sizes = kernel_sizes\r\n+        self.window_size = window_size\r\n+        self.group_split = group_split\r\n+        convs = []\r\n+        act_blocks = []\r\n+        qkvs = []\r\n+        #projs = []\r\n+        for i in range(len(kernel_sizes)):\r\n+            kernel_size = kernel_sizes[i]\r\n+            group_head = group_split[i]\r\n+            if group_head == 0:\r\n+                continue\r\n+            convs.append(nn.Conv2d(3*self.dim_head*group_head, 3*self.dim_head*group_head, kernel_size,\r\n+                         1, kernel_size//2, groups=3*self.dim_head*group_head))\r\n+            act_blocks.append(AttnMap(self.dim_head*group_head))\r\n+            qkvs.append(nn.Conv2d(dim, 3*group_head*self.dim_head, 1, 1, 0, bias=qkv_bias))\r\n+            #projs.append(nn.Linear(group_head*self.dim_head, group_head*self.dim_head, bias=qkv_bias))\r\n+        if group_split[-1] != 0:\r\n+            self.global_q = nn.Conv2d(dim, group_split[-1]*self.dim_head, 1, 1, 0, bias=qkv_bias)\r\n+            self.global_kv = nn.Conv2d(dim, group_split[-1]*self.dim_head*2, 1, 1, 0, bias=qkv_bias)\r\n+            #self.global_proj = nn.Linear(group_split[-1]*self.dim_head, group_split[-1]*self.dim_head, bias=qkv_bias)\r\n+            self.avgpool = nn.AvgPool2d(window_size, window_size) if window_size!=1 else nn.Identity()\r\n+\r\n+        self.convs = nn.ModuleList(convs)\r\n+        self.act_blocks = nn.ModuleList(act_blocks)\r\n+        self.qkvs = nn.ModuleList(qkvs)\r\n+        self.proj = nn.Conv2d(dim, dim, 1, 1, 0, bias=qkv_bias)\r\n+        self.attn_drop = nn.Dropout(attn_drop)\r\n+        self.proj_drop = nn.Dropout(proj_drop)\r\n+\r\n+    def high_fre_attntion(self, x: paddle.Tensor, to_qkv: nn.Layer, mixer: nn.Layer, attn_block: nn.Layer):\r\n+        '''\r\n+        x: (b c h w)\r\n+        '''\r\n+        b, c, h, w = x.size()\r\n+        qkv = to_qkv(x) #(b (3 m d) h w)\r\n+        qkv = mixer(qkv).reshape(b, 3, -1, h, w).transpose(0, 1).contiguous() #(3 b (m d) h w)\r\n+        q, k, v = qkv #(b (m d) h w)\r\n+        attn = attn_block(q.mul(k)).mul(self.scalor)\r\n+        attn = self.attn_drop(paddle.tanh(attn))\r\n+        res = attn.mul(v) #(b (m d) h w)\r\n+        return res\r\n+        \r\n+    def low_fre_attention(self, x : paddle.Tensor, to_q: nn.layer, to_kv: nn.Layer, avgpool: nn.Layer):\r\n+        '''\r\n+        x: (b c h w)\r\n+        '''\r\n+        b, c, h, w = x.size()\r\n+        \r\n+        q = to_q(x).reshape(b, -1, self.dim_head, h*w).transpose(-1, -2).contiguous() #(b m (h w) d)\r\n+        kv = avgpool(x) #(b c h w)\r\n+        kv = to_kv(kv).view(b, 2, -1, self.dim_head, (h*w)//(self.window_size**2)).permute(1, 0, 2, 4, 3).contiguous() #(2 b m (H W) d)\r\n+        k, v = kv #(b m (H W) d)\r\n+        attn = self.scalor * q @ k.transpose(-1, -2) #(b m (h w) (H W))\r\n+        attn = self.attn_drop(attn.softmax(dim=-1))\r\n+        res = attn @ v #(b m (h w) d)\r\n+        res = res.transpose(2, 3).reshape(b, -1, h, w).contiguous()\r\n+        return res\r\n+\r\n+    def forward(self, x: paddle.Tensor):\r\n+        '''\r\n+        x: (b c h w)\r\n+        '''\r\n+        res = []\r\n+        for i in range(len(self.kernel_sizes)):\r\n+            if self.group_split[i] == 0:\r\n+                continue\r\n+            res.append(self.high_fre_attntion(x, self.qkvs[i], self.convs[i], self.act_blocks[i]))\r\n+        if self.group_split[-1] != 0:\r\n+            res.append(self.low_fre_attention(x, self.global_q, self.global_kv, self.avgpool))\r\n+        return self.proj_drop(self.proj(paddle.cat(res, dim=1)))\r\n+\r\n+\r\n+class ConvFFN(nn.Layer):\r\n+\r\n+    def __init__(self, in_channels, hidden_channels, kernel_size, stride,\r\n+                 out_channels, act_layer=nn.GELU, drop_out=0.):\r\n+        super().__init__()\r\n+        self.fc1 = nn.Conv2D(in_channels, hidden_channels, 1, 1, 0)\r\n+        self.act = act_layer()\r\n+        self.dwconv = nn.Conv2D(hidden_channels, hidden_channels, kernel_size, stride, \r\n+                                kernel_size//2, groups=hidden_channels)\r\n+        self.fc2 = nn.Conv2D(hidden_channels, out_channels, 1, 1, 0)\r\n+        self.drop = nn.Dropout(drop_out)\r\n+\r\n+    def forward(self, x: paddle.Tensor):\r\n+        '''\r\n+        x: (b h w c)\r\n+        '''\r\n+        x = self.fc1(x)\r\n+        x = self.act(x)\r\n+        x = self.dwconv(x)\r\n+        x = self.drop(x)\r\n+        x = self.fc2(x)\r\n+        x = self.drop(x)\r\n+        return x\r\n+\r\n+\r\n+\r\n+class EfficientBlock(nn.Layer):\r\n+\r\n+    def __init__(self, dim, out_dim, num_heads, group_split: List[int], kernel_sizes: List[int], window_size: int,\r\n+                 mlp_kernel_size: int, mlp_ratio: int, stride: int, attn_drop=0., mlp_drop=0., qkv_bias=True,\r\n+                 drop_path=0.):\r\n+        super().__init__()\r\n+        self.dim = dim\r\n+        self.mlp_ratio = mlp_ratio\r\n+        self.norm1 = nn.GroupNorm(1, dim)\r\n+        self.attn = EfficientAttention(dim, num_heads, group_split, kernel_sizes, window_size,\r\n+                                       attn_drop, mlp_drop, qkv_bias)\r\n+        self.drop_path = DropPath(drop_path)\r\n+        self.norm2 = nn.GroupNorm(1, dim)\r\n+        mlp_hidden_dim = int(dim * mlp_ratio)\r\n+        self.stride = stride\r\n+        if stride == 1:\r\n+            self.downsample = nn.Identity()\r\n+        else:\r\n+            self.downsample = nn.Sequential(\r\n+                                nn.Conv2d(dim, dim, mlp_kernel_size, 2, mlp_kernel_size//2),\r\n+                                nn.SyncBatchNorm(dim),\r\n+                                nn.Conv2d(dim, out_dim, 1, 1, 0),\r\n+                            )\r\n+        self.mlp = ConvFFN(dim, mlp_hidden_dim, mlp_kernel_size, stride, out_dim, \r\n+                        drop_out=mlp_drop)\r\n+    def forward(self, x: paddle.Tensor):\r\n+        x = x + self.drop_path(self.attn(self.norm1(x)))\r\n+        x = self.downsample(x) + self.drop_path(self.mlp(self.norm2(x)))\r\n+        return x\r\n+\r\n+if __name__ == '__main__':\r\n+    input = paddle.randn(4, 96, 56, 56)\r\n+    model = EfficientBlock(96, 192, 3, [1, 1, 1], [7, 5], 7, 7, 4, 2)\r\n+    print(model(input).size())\n\\ No newline at end of file\n"
                },
                {
                    "date": 1709198174493,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -209,7 +209,7 @@\n         x = self.downsample(x) + self.drop_path(self.mlp(self.norm2(x)))\r\n         return x\r\n \r\n if __name__ == '__main__':\r\n-    input = paddle.randn(4, 96, 56, 56)\r\n+    input = paddle.randn((4, 96, 56, 56))\r\n     model = EfficientBlock(96, 192, 3, [1, 1, 1], [7, 5], 7, 7, 4, 2)\r\n     print(model(input).size())\n\\ No newline at end of file\n"
                },
                {
                    "date": 1709198319642,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -91,23 +91,23 @@\n             kernel_size = kernel_sizes[i]\r\n             group_head = group_split[i]\r\n             if group_head == 0:\r\n                 continue\r\n-            convs.append(nn.Conv2d(3*self.dim_head*group_head, 3*self.dim_head*group_head, kernel_size,\r\n+            convs.append(nn.Conv2D(3*self.dim_head*group_head, 3*self.dim_head*group_head, kernel_size,\r\n                          1, kernel_size//2, groups=3*self.dim_head*group_head))\r\n             act_blocks.append(AttnMap(self.dim_head*group_head))\r\n-            qkvs.append(nn.Conv2d(dim, 3*group_head*self.dim_head, 1, 1, 0, bias=qkv_bias))\r\n+            qkvs.append(nn.Conv2D(dim, 3*group_head*self.dim_head, 1, 1, 0, bias=qkv_bias))\r\n             #projs.append(nn.Linear(group_head*self.dim_head, group_head*self.dim_head, bias=qkv_bias))\r\n         if group_split[-1] != 0:\r\n-            self.global_q = nn.Conv2d(dim, group_split[-1]*self.dim_head, 1, 1, 0, bias=qkv_bias)\r\n-            self.global_kv = nn.Conv2d(dim, group_split[-1]*self.dim_head*2, 1, 1, 0, bias=qkv_bias)\r\n+            self.global_q = nn.Conv2D(dim, group_split[-1]*self.dim_head, 1, 1, 0, bias=qkv_bias)\r\n+            self.global_kv = nn.Conv2D(dim, group_split[-1]*self.dim_head*2, 1, 1, 0, bias=qkv_bias)\r\n             #self.global_proj = nn.Linear(group_split[-1]*self.dim_head, group_split[-1]*self.dim_head, bias=qkv_bias)\r\n             self.avgpool = nn.AvgPool2d(window_size, window_size) if window_size!=1 else nn.Identity()\r\n \r\n         self.convs = nn.ModuleList(convs)\r\n         self.act_blocks = nn.ModuleList(act_blocks)\r\n         self.qkvs = nn.ModuleList(qkvs)\r\n-        self.proj = nn.Conv2d(dim, dim, 1, 1, 0, bias=qkv_bias)\r\n+        self.proj = nn.Conv2D(dim, dim, 1, 1, 0, bias=qkv_bias)\r\n         self.attn_drop = nn.Dropout(attn_drop)\r\n         self.proj_drop = nn.Dropout(proj_drop)\r\n \r\n     def high_fre_attntion(self, x: paddle.Tensor, to_qkv: nn.Layer, mixer: nn.Layer, attn_block: nn.Layer):\r\n@@ -197,11 +197,11 @@\n         if stride == 1:\r\n             self.downsample = nn.Identity()\r\n         else:\r\n             self.downsample = nn.Sequential(\r\n-                                nn.Conv2d(dim, dim, mlp_kernel_size, 2, mlp_kernel_size//2),\r\n+                                nn.Conv2D(dim, dim, mlp_kernel_size, 2, mlp_kernel_size//2),\r\n                                 nn.SyncBatchNorm(dim),\r\n-                                nn.Conv2d(dim, out_dim, 1, 1, 0),\r\n+                                nn.Conv2D(dim, out_dim, 1, 1, 0),\r\n                             )\r\n         self.mlp = ConvFFN(dim, mlp_hidden_dim, mlp_kernel_size, stride, out_dim, \r\n                         drop_out=mlp_drop)\r\n     def forward(self, x: paddle.Tensor):\r\n"
                },
                {
                    "date": 1709198729199,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -94,20 +94,20 @@\n                 continue\r\n             convs.append(nn.Conv2D(3*self.dim_head*group_head, 3*self.dim_head*group_head, kernel_size,\r\n                          1, kernel_size//2, groups=3*self.dim_head*group_head))\r\n             act_blocks.append(AttnMap(self.dim_head*group_head))\r\n-            qkvs.append(nn.Conv2D(dim, 3*group_head*self.dim_head, 1, 1, 0, bias=qkv_bias))\r\n+            qkvs.append(nn.Conv2D(dim, 3*group_head*self.dim_head, 1, 1, 0, bias_attr=qkv_bias))\r\n             #projs.append(nn.Linear(group_head*self.dim_head, group_head*self.dim_head, bias=qkv_bias))\r\n         if group_split[-1] != 0:\r\n-            self.global_q = nn.Conv2D(dim, group_split[-1]*self.dim_head, 1, 1, 0, bias=qkv_bias)\r\n-            self.global_kv = nn.Conv2D(dim, group_split[-1]*self.dim_head*2, 1, 1, 0, bias=qkv_bias)\r\n+            self.global_q = nn.Conv2D(dim, group_split[-1]*self.dim_head, 1, 1, 0, bias_attr=qkv_bias)\r\n+            self.global_kv = nn.Conv2D(dim, group_split[-1]*self.dim_head*2, 1, 1, 0, bias_attr=qkv_bias)\r\n             #self.global_proj = nn.Linear(group_split[-1]*self.dim_head, group_split[-1]*self.dim_head, bias=qkv_bias)\r\n             self.avgpool = nn.AvgPool2d(window_size, window_size) if window_size!=1 else nn.Identity()\r\n \r\n         self.convs = nn.ModuleList(convs)\r\n         self.act_blocks = nn.ModuleList(act_blocks)\r\n         self.qkvs = nn.ModuleList(qkvs)\r\n-        self.proj = nn.Conv2D(dim, dim, 1, 1, 0, bias=qkv_bias)\r\n+        self.proj = nn.Conv2D(dim, dim, 1, 1, 0, bias_attr=qkv_bias)\r\n         self.attn_drop = nn.Dropout(attn_drop)\r\n         self.proj_drop = nn.Dropout(proj_drop)\r\n \r\n     def high_fre_attntion(self, x: paddle.Tensor, to_qkv: nn.Layer, mixer: nn.Layer, attn_block: nn.Layer):\r\n"
                },
                {
                    "date": 1709198756965,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -100,9 +100,9 @@\n         if group_split[-1] != 0:\r\n             self.global_q = nn.Conv2D(dim, group_split[-1]*self.dim_head, 1, 1, 0, bias_attr=qkv_bias)\r\n             self.global_kv = nn.Conv2D(dim, group_split[-1]*self.dim_head*2, 1, 1, 0, bias_attr=qkv_bias)\r\n             #self.global_proj = nn.Linear(group_split[-1]*self.dim_head, group_split[-1]*self.dim_head, bias=qkv_bias)\r\n-            self.avgpool = nn.AvgPool2d(window_size, window_size) if window_size!=1 else nn.Identity()\r\n+            self.avgpool = nn.AvgPool2D(window_size, window_size) if window_size!=1 else nn.Identity()\r\n \r\n         self.convs = nn.ModuleList(convs)\r\n         self.act_blocks = nn.ModuleList(act_blocks)\r\n         self.qkvs = nn.ModuleList(qkvs)\r\n"
                },
                {
                    "date": 1709198854688,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -102,11 +102,11 @@\n             self.global_kv = nn.Conv2D(dim, group_split[-1]*self.dim_head*2, 1, 1, 0, bias_attr=qkv_bias)\r\n             #self.global_proj = nn.Linear(group_split[-1]*self.dim_head, group_split[-1]*self.dim_head, bias=qkv_bias)\r\n             self.avgpool = nn.AvgPool2D(window_size, window_size) if window_size!=1 else nn.Identity()\r\n \r\n-        self.convs = nn.ModuleList(convs)\r\n-        self.act_blocks = nn.ModuleList(act_blocks)\r\n-        self.qkvs = nn.ModuleList(qkvs)\r\n+        self.convs = nn.LayerList(convs)\r\n+        self.act_blocks = nn.LayerList(act_blocks)\r\n+        self.qkvs = nn.LayerList(qkvs)\r\n         self.proj = nn.Conv2D(dim, dim, 1, 1, 0, bias_attr=qkv_bias)\r\n         self.attn_drop = nn.Dropout(attn_drop)\r\n         self.proj_drop = nn.Dropout(proj_drop)\r\n \r\n"
                },
                {
                    "date": 1709198990731,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -209,7 +209,7 @@\n         x = self.downsample(x) + self.drop_path(self.mlp(self.norm2(x)))\r\n         return x\r\n \r\n if __name__ == '__main__':\r\n-    input = paddle.randn((4, 96, 56, 56))\r\n+    input = paddle.randn((4, 96, 56, 56),dtype=float32)\r\n     model = EfficientBlock(96, 192, 3, [1, 1, 1], [7, 5], 7, 7, 4, 2)\r\n     print(model(input).size())\n\\ No newline at end of file\n"
                },
                {
                    "date": 1709199001543,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -209,7 +209,7 @@\n         x = self.downsample(x) + self.drop_path(self.mlp(self.norm2(x)))\r\n         return x\r\n \r\n if __name__ == '__main__':\r\n-    input = paddle.randn((4, 96, 56, 56),dtype=float32)\r\n+    input = paddle.randn((4, 96, 56, 56),dtype=\"float32\")\r\n     model = EfficientBlock(96, 192, 3, [1, 1, 1], [7, 5], 7, 7, 4, 2)\r\n     print(model(input).size())\n\\ No newline at end of file\n"
                },
                {
                    "date": 1709199044843,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -113,8 +113,9 @@\n     def high_fre_attntion(self, x: paddle.Tensor, to_qkv: nn.Layer, mixer: nn.Layer, attn_block: nn.Layer):\r\n         '''\r\n         x: (b c h w)\r\n         '''\r\n+        print(x)\r\n         b, c, h, w = x.size()\r\n         qkv = to_qkv(x) #(b (3 m d) h w)\r\n         qkv = mixer(qkv).reshape(b, 3, -1, h, w).transpose(0, 1).contiguous() #(3 b (m d) h w)\r\n         q, k, v = qkv #(b (m d) h w)\r\n"
                },
                {
                    "date": 1709199069912,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -114,9 +114,9 @@\n         '''\r\n         x: (b c h w)\r\n         '''\r\n         print(x)\r\n-        b, c, h, w = x.size()\r\n+        b, c, h, w = x.shape()\r\n         qkv = to_qkv(x) #(b (3 m d) h w)\r\n         qkv = mixer(qkv).reshape(b, 3, -1, h, w).transpose(0, 1).contiguous() #(3 b (m d) h w)\r\n         q, k, v = qkv #(b (m d) h w)\r\n         attn = attn_block(q.mul(k)).mul(self.scalor)\r\n"
                },
                {
                    "date": 1709199325404,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -114,9 +114,9 @@\n         '''\r\n         x: (b c h w)\r\n         '''\r\n         print(x)\r\n-        b, c, h, w = x.shape()\r\n+        b, c, h, w = x.size()\r\n         qkv = to_qkv(x) #(b (3 m d) h w)\r\n         qkv = mixer(qkv).reshape(b, 3, -1, h, w).transpose(0, 1).contiguous() #(3 b (m d) h w)\r\n         q, k, v = qkv #(b (m d) h w)\r\n         attn = attn_block(q.mul(k)).mul(self.scalor)\r\n@@ -127,8 +127,9 @@\n     def low_fre_attention(self, x : paddle.Tensor, to_q: nn.layer, to_kv: nn.Layer, avgpool: nn.Layer):\r\n         '''\r\n         x: (b c h w)\r\n         '''\r\n+        \r\n         b, c, h, w = x.size()\r\n         \r\n         q = to_q(x).reshape(b, -1, self.dim_head, h*w).transpose(-1, -2).contiguous() #(b m (h w) d)\r\n         kv = avgpool(x) #(b c h w)\r\n"
                },
                {
                    "date": 1709199333758,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -113,9 +113,9 @@\n     def high_fre_attntion(self, x: paddle.Tensor, to_qkv: nn.Layer, mixer: nn.Layer, attn_block: nn.Layer):\r\n         '''\r\n         x: (b c h w)\r\n         '''\r\n-        print(x)\r\n+        print(x.size)\r\n         b, c, h, w = x.size()\r\n         qkv = to_qkv(x) #(b (3 m d) h w)\r\n         qkv = mixer(qkv).reshape(b, 3, -1, h, w).transpose(0, 1).contiguous() #(3 b (m d) h w)\r\n         q, k, v = qkv #(b (m d) h w)\r\n"
                },
                {
                    "date": 1709199557950,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -114,9 +114,9 @@\n         '''\r\n         x: (b c h w)\r\n         '''\r\n         print(x.size)\r\n-        b, c, h, w = x.size()\r\n+        b, c, h, w = x.shape()[:]\r\n         qkv = to_qkv(x) #(b (3 m d) h w)\r\n         qkv = mixer(qkv).reshape(b, 3, -1, h, w).transpose(0, 1).contiguous() #(3 b (m d) h w)\r\n         q, k, v = qkv #(b (m d) h w)\r\n         attn = attn_block(q.mul(k)).mul(self.scalor)\r\n@@ -128,9 +128,9 @@\n         '''\r\n         x: (b c h w)\r\n         '''\r\n         \r\n-        b, c, h, w = x.size()\r\n+        b, c, h, w = x.shape()[:]\r\n         \r\n         q = to_q(x).reshape(b, -1, self.dim_head, h*w).transpose(-1, -2).contiguous() #(b m (h w) d)\r\n         kv = avgpool(x) #(b c h w)\r\n         kv = to_kv(kv).view(b, 2, -1, self.dim_head, (h*w)//(self.window_size**2)).permute(1, 0, 2, 4, 3).contiguous() #(2 b m (H W) d)\r\n"
                },
                {
                    "date": 1709199621164,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -114,9 +114,9 @@\n         '''\r\n         x: (b c h w)\r\n         '''\r\n         print(x.size)\r\n-        b, c, h, w = x.shape()[:]\r\n+        b, c, h, w = x.shape\r\n         qkv = to_qkv(x) #(b (3 m d) h w)\r\n         qkv = mixer(qkv).reshape(b, 3, -1, h, w).transpose(0, 1).contiguous() #(3 b (m d) h w)\r\n         q, k, v = qkv #(b (m d) h w)\r\n         attn = attn_block(q.mul(k)).mul(self.scalor)\r\n@@ -128,9 +128,9 @@\n         '''\r\n         x: (b c h w)\r\n         '''\r\n         \r\n-        b, c, h, w = x.shape()[:]\r\n+        b, c, h, w = x.shape\r\n         \r\n         q = to_q(x).reshape(b, -1, self.dim_head, h*w).transpose(-1, -2).contiguous() #(b m (h w) d)\r\n         kv = avgpool(x) #(b c h w)\r\n         kv = to_kv(kv).view(b, 2, -1, self.dim_head, (h*w)//(self.window_size**2)).permute(1, 0, 2, 4, 3).contiguous() #(2 b m (H W) d)\r\n"
                },
                {
                    "date": 1709200951892,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -211,7 +211,13 @@\n         x = self.downsample(x) + self.drop_path(self.mlp(self.norm2(x)))\r\n         return x\r\n \r\n if __name__ == '__main__':\r\n-    input = paddle.randn((4, 96, 56, 56),dtype=\"float32\")\r\n-    model = EfficientBlock(96, 192, 3, [1, 1, 1], [7, 5], 7, 7, 4, 2)\r\n-    print(model(input).size())\n\\ No newline at end of file\n+    # input = paddle.randn((4, 96, 56, 56),dtype=\"float32\")\r\n+    # model = EfficientBlock(96, 192, 3, [1, 1, 1], [7, 5], 7, 7, 4, 2)\r\n+    # print(model(input).size())\r\n+    \r\n+    import paddle\r\n+    print(paddle.__version__)\r\n+\r\n+    print(paddle.version.cuda)\r\n+    print(paddle.backends.cudnn.version())\r\n"
                },
                {
                    "date": 1709200970115,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -219,5 +219,5 @@\n     import paddle\r\n     print(paddle.__version__)\r\n \r\n     print(paddle.version.cuda)\r\n-    print(paddle.backends.cudnn.version())\r\n+    print(paddle.cudnn.version())\r\n"
                },
                {
                    "date": 1709200997992,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -219,5 +219,5 @@\n     import paddle\r\n     print(paddle.__version__)\r\n \r\n     print(paddle.version.cuda)\r\n-    print(paddle.cudnn.version())\r\n+    print(paddle.get_cudnn_version())\r\n"
                },
                {
                    "date": 1709201014806,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -218,6 +218,6 @@\n     \r\n     import paddle\r\n     print(paddle.__version__)\r\n \r\n-    print(paddle.version.cuda)\r\n+    print(paddle.version.cuda())\r\n     print(paddle.get_cudnn_version())\r\n"
                },
                {
                    "date": 1709201233202,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -215,9 +215,12 @@\n     # input = paddle.randn((4, 96, 56, 56),dtype=\"float32\")\r\n     # model = EfficientBlock(96, 192, 3, [1, 1, 1], [7, 5], 7, 7, 4, 2)\r\n     # print(model(input).size())\r\n     \r\n+\r\n     import paddle\r\n     print(paddle.__version__)\r\n-\r\n-    print(paddle.version.cuda())\r\n-    print(paddle.get_cudnn_version())\r\n+    print(paddle.device.get_device())\r\n+    print(paddle.device.get_cudnn_version())\r\n+    import paddle.fluid\r\n+    paddle.fluid.install_check.run_check()\r\n+    exit()\r\n"
                },
                {
                    "date": 1709201642203,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -211,16 +211,16 @@\n         x = self.downsample(x) + self.drop_path(self.mlp(self.norm2(x)))\r\n         return x\r\n \r\n if __name__ == '__main__':\r\n-    # input = paddle.randn((4, 96, 56, 56),dtype=\"float32\")\r\n-    # model = EfficientBlock(96, 192, 3, [1, 1, 1], [7, 5], 7, 7, 4, 2)\r\n-    # print(model(input).size())\r\n+    input = paddle.randn((4, 96, 56, 56),dtype=\"float32\")\r\n+    model = EfficientBlock(96, 192, 3, [1, 1, 1], [7, 5], 7, 7, 4, 2)\r\n+    print(model(input).size())\r\n     \r\n \r\n-    import paddle\r\n-    print(paddle.__version__)\r\n-    print(paddle.device.get_device())\r\n-    print(paddle.device.get_cudnn_version())\r\n-    import paddle.fluid\r\n-    paddle.fluid.install_check.run_check()\r\n-    exit()\r\n+    # import paddle\r\n+    # print(paddle.__version__)\r\n+    # print(paddle.device.get_device())\r\n+    # print(paddle.device.get_cudnn_version())\r\n+    # import paddle.fluid\r\n+    # paddle.fluid.install_check.run_check()\r\n+    # exit()\r\n"
                },
                {
                    "date": 1709202068415,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -211,16 +211,16 @@\n         x = self.downsample(x) + self.drop_path(self.mlp(self.norm2(x)))\r\n         return x\r\n \r\n if __name__ == '__main__':\r\n-    input = paddle.randn((4, 96, 56, 56),dtype=\"float32\")\r\n-    model = EfficientBlock(96, 192, 3, [1, 1, 1], [7, 5], 7, 7, 4, 2)\r\n-    print(model(input).size())\r\n+    # input = paddle.randn((4, 96, 56, 56),dtype=\"float32\")\r\n+    # model = EfficientBlock(96, 192, 3, [1, 1, 1], [7, 5], 7, 7, 4, 2)\r\n+    # print(model(input).size())\r\n     \r\n \r\n-    # import paddle\r\n-    # print(paddle.__version__)\r\n-    # print(paddle.device.get_device())\r\n-    # print(paddle.device.get_cudnn_version())\r\n-    # import paddle.fluid\r\n-    # paddle.fluid.install_check.run_check()\r\n-    # exit()\r\n+    import paddle\r\n+    print(paddle.__version__)\r\n+    print(paddle.device.get_device())\r\n+    print(paddle.device.get_cudnn_version())\r\n+    import paddle.fluid\r\n+    paddle.fluid.install_check.run_check()\r\n+    exit()\r\n"
                },
                {
                    "date": 1709203808652,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,226 @@\n+from typing import List\r\n+\r\n+import math\r\n+from functools import partial\r\n+\r\n+import paddle\r\n+from paddle import nn\r\n+import paddle.nn.functional as F\r\n+from paddle.autograd import PyLayer\r\n+\r\n+##Swish激活函数  由之前的激活函数复合而成出来的   \r\n+##通过创建 PyLayer 子类的方式实现Python端自定义算子\r\n+class SwishImplementation(PyLayer):\r\n+    def forward(ctx, i):\r\n+        result = i * F.sigmoid(i)\r\n+        ctx.save_for_backward(i)\r\n+        return result\r\n+\r\n+    def backward(ctx, grad_output):\r\n+        i = ctx.saved_variables[0]\r\n+        sigmoid_i = F.sigmoid(i)\r\n+        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\r\n+\r\n+class MemoryEfficientSwish(nn.Layer):\r\n+    def forward(self, x):\r\n+        return SwishImplementation.apply(x)\r\n+\r\n+\r\n+def drop_path(x, drop_prob=0., training=False):\r\n+    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\r\n+    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\r\n+    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ...\r\n+    \"\"\"\r\n+    if drop_prob == 0. or not training:\r\n+        return x\r\n+    keep_prob = paddle.to_tensor(1 - drop_prob)\r\n+    shape = (paddle.shape(x)[0], ) + (1, ) * (x.ndim - 1)\r\n+    random_tensor = keep_prob + paddle.rand(shape, dtype=x.dtype)\r\n+    random_tensor = paddle.floor(random_tensor)  # binarize\r\n+    output = x.divide(keep_prob) * random_tensor\r\n+    return output\r\n+\r\n+\r\n+class DropPath(nn.Layer):\r\n+    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\r\n+    \"\"\"\r\n+\r\n+    def __init__(self, drop_prob=None):\r\n+        super(DropPath, self).__init__()\r\n+        self.drop_prob = drop_prob\r\n+\r\n+    def forward(self, x):\r\n+        return drop_path(x, self.drop_prob, self.training)\r\n+    \r\n+    def extra_repr(self):\r\n+        return f'drop_prob={round(self.drop_prob,3):0.3f}'\r\n+\r\n+\r\n+\r\n+class AttnMap(nn.Layer):\r\n+    def __init__(self, dim):\r\n+        super().__init__()\r\n+        self.act_block = nn.Sequential(\r\n+                            nn.Conv2D(dim, dim, 1, 1, 0),\r\n+                            MemoryEfficientSwish(),\r\n+                            nn.Conv2D(dim, dim, 1, 1, 0)\r\n+                            #nn.Identity()\r\n+                         )\r\n+    def forward(self, x):\r\n+        return self.act_block(x)\r\n+    \r\n+class EfficientAttention(nn.Layer):\r\n+\r\n+    def __init__(self, dim, num_heads, group_split: List[int], kernel_sizes: List[int], window_size=7, \r\n+                 attn_drop=0., proj_drop=0., qkv_bias=True):\r\n+        super().__init__()\r\n+        assert sum(group_split) == num_heads\r\n+        assert len(kernel_sizes) + 1 == len(group_split)\r\n+        self.dim = dim\r\n+        self.num_heads = num_heads\r\n+        self.dim_head = dim // num_heads\r\n+        self.scalor = self.dim_head ** -0.5\r\n+        self.kernel_sizes = kernel_sizes\r\n+        self.window_size = window_size\r\n+        self.group_split = group_split\r\n+        convs = []\r\n+        act_blocks = []\r\n+        qkvs = []\r\n+        #projs = []\r\n+        for i in range(len(kernel_sizes)):\r\n+            kernel_size = kernel_sizes[i]\r\n+            group_head = group_split[i]\r\n+            if group_head == 0:\r\n+                continue\r\n+            convs.append(nn.Conv2D(3*self.dim_head*group_head, 3*self.dim_head*group_head, kernel_size,\r\n+                         1, kernel_size//2, groups=3*self.dim_head*group_head))\r\n+            act_blocks.append(AttnMap(self.dim_head*group_head))\r\n+            qkvs.append(nn.Conv2D(dim, 3*group_head*self.dim_head, 1, 1, 0, bias_attr=qkv_bias))\r\n+            #projs.append(nn.Linear(group_head*self.dim_head, group_head*self.dim_head, bias=qkv_bias))\r\n+        if group_split[-1] != 0:\r\n+            self.global_q = nn.Conv2D(dim, group_split[-1]*self.dim_head, 1, 1, 0, bias_attr=qkv_bias)\r\n+            self.global_kv = nn.Conv2D(dim, group_split[-1]*self.dim_head*2, 1, 1, 0, bias_attr=qkv_bias)\r\n+            #self.global_proj = nn.Linear(group_split[-1]*self.dim_head, group_split[-1]*self.dim_head, bias=qkv_bias)\r\n+            self.avgpool = nn.AvgPool2D(window_size, window_size) if window_size!=1 else nn.Identity()\r\n+\r\n+        self.convs = nn.LayerList(convs)\r\n+        self.act_blocks = nn.LayerList(act_blocks)\r\n+        self.qkvs = nn.LayerList(qkvs)\r\n+        self.proj = nn.Conv2D(dim, dim, 1, 1, 0, bias_attr=qkv_bias)\r\n+        self.attn_drop = nn.Dropout(attn_drop)\r\n+        self.proj_drop = nn.Dropout(proj_drop)\r\n+\r\n+    def high_fre_attntion(self, x: paddle.Tensor, to_qkv: nn.Layer, mixer: nn.Layer, attn_block: nn.Layer):\r\n+        '''\r\n+        x: (b c h w)\r\n+        '''\r\n+        print(x.size)\r\n+        b, c, h, w = x.shape\r\n+        qkv = to_qkv(x) #(b (3 m d) h w)\r\n+        qkv = mixer(qkv).reshape(b, 3, -1, h, w).transpose(0, 1).contiguous() #(3 b (m d) h w)\r\n+        q, k, v = qkv #(b (m d) h w)\r\n+        attn = attn_block(q.mul(k)).mul(self.scalor)\r\n+        attn = self.attn_drop(paddle.tanh(attn))\r\n+        res = attn.mul(v) #(b (m d) h w)\r\n+        return res\r\n+        \r\n+    def low_fre_attention(self, x : paddle.Tensor, to_q: nn.layer, to_kv: nn.Layer, avgpool: nn.Layer):\r\n+        '''\r\n+        x: (b c h w)\r\n+        '''\r\n+        \r\n+        b, c, h, w = x.shape\r\n+        \r\n+        q = to_q(x).reshape(b, -1, self.dim_head, h*w).transpose(-1, -2).contiguous() #(b m (h w) d)\r\n+        kv = avgpool(x) #(b c h w)\r\n+        kv = to_kv(kv).view(b, 2, -1, self.dim_head, (h*w)//(self.window_size**2)).permute(1, 0, 2, 4, 3).contiguous() #(2 b m (H W) d)\r\n+        k, v = kv #(b m (H W) d)\r\n+        attn = self.scalor * q @ k.transpose(-1, -2) #(b m (h w) (H W))\r\n+        attn = self.attn_drop(attn.softmax(dim=-1))\r\n+        res = attn @ v #(b m (h w) d)\r\n+        res = res.transpose(2, 3).reshape(b, -1, h, w).contiguous()\r\n+        return res\r\n+\r\n+    def forward(self, x: paddle.Tensor):\r\n+        '''\r\n+        x: (b c h w)\r\n+        '''\r\n+        res = []\r\n+        for i in range(len(self.kernel_sizes)):\r\n+            if self.group_split[i] == 0:\r\n+                continue\r\n+            res.append(self.high_fre_attntion(x, self.qkvs[i], self.convs[i], self.act_blocks[i]))\r\n+        if self.group_split[-1] != 0:\r\n+            res.append(self.low_fre_attention(x, self.global_q, self.global_kv, self.avgpool))\r\n+        return self.proj_drop(self.proj(paddle.cat(res, dim=1)))\r\n+\r\n+\r\n+class ConvFFN(nn.Layer):\r\n+\r\n+    def __init__(self, in_channels, hidden_channels, kernel_size, stride,\r\n+                 out_channels, act_layer=nn.GELU, drop_out=0.):\r\n+        super().__init__()\r\n+        self.fc1 = nn.Conv2D(in_channels, hidden_channels, 1, 1, 0)\r\n+        self.act = act_layer()\r\n+        self.dwconv = nn.Conv2D(hidden_channels, hidden_channels, kernel_size, stride, \r\n+                                kernel_size//2, groups=hidden_channels)\r\n+        self.fc2 = nn.Conv2D(hidden_channels, out_channels, 1, 1, 0)\r\n+        self.drop = nn.Dropout(drop_out)\r\n+\r\n+    def forward(self, x: paddle.Tensor):\r\n+        '''\r\n+        x: (b h w c)\r\n+        '''\r\n+        x = self.fc1(x)\r\n+        x = self.act(x)\r\n+        x = self.dwconv(x)\r\n+        x = self.drop(x)\r\n+        x = self.fc2(x)\r\n+        x = self.drop(x)\r\n+        return x\r\n+\r\n+\r\n+\r\n+class EfficientBlock(nn.Layer):\r\n+\r\n+    def __init__(self, dim, out_dim, num_heads, group_split: List[int], kernel_sizes: List[int], window_size: int,\r\n+                 mlp_kernel_size: int, mlp_ratio: int, stride: int, attn_drop=0., mlp_drop=0., qkv_bias=True,\r\n+                 drop_path=0.):\r\n+        super().__init__()\r\n+        self.dim = dim\r\n+        self.mlp_ratio = mlp_ratio\r\n+        self.norm1 = nn.GroupNorm(1, dim)\r\n+        self.attn = EfficientAttention(dim, num_heads, group_split, kernel_sizes, window_size,\r\n+                                       attn_drop, mlp_drop, qkv_bias)\r\n+        self.drop_path = DropPath(drop_path)\r\n+        self.norm2 = nn.GroupNorm(1, dim)\r\n+        mlp_hidden_dim = int(dim * mlp_ratio)\r\n+        self.stride = stride\r\n+        if stride == 1:\r\n+            self.downsample = nn.Identity()\r\n+        else:\r\n+            self.downsample = nn.Sequential(\r\n+                                nn.Conv2D(dim, dim, mlp_kernel_size, 2, mlp_kernel_size//2),\r\n+                                nn.SyncBatchNorm(dim),\r\n+                                nn.Conv2D(dim, out_dim, 1, 1, 0),\r\n+                            )\r\n+        self.mlp = ConvFFN(dim, mlp_hidden_dim, mlp_kernel_size, stride, out_dim, \r\n+                        drop_out=mlp_drop)\r\n+    def forward(self, x: paddle.Tensor):\r\n+        x = x + self.drop_path(self.attn(self.norm1(x)))\r\n+        x = self.downsample(x) + self.drop_path(self.mlp(self.norm2(x)))\r\n+        return x\r\n+\r\n+if __name__ == '__main__':\r\n+    # input = paddle.randn((4, 96, 56, 56),dtype=\"float32\")\r\n+    # model = EfficientBlock(96, 192, 3, [1, 1, 1], [7, 5], 7, 7, 4, 2)\r\n+    # print(model(input).size())\r\n+    \r\n+\r\n+    import paddle\r\n+    print(paddle.__version__)\r\n+    print(paddle.version.cuda())\r\n+    print(paddle.device.get_cudnn_version())\r\n+    import paddle.fluid\r\n+    paddle.fluid.install_check.run_check()\r\n+    exit()\r\n"
                },
                {
                    "date": 1709203862484,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -211,242 +211,16 @@\n         x = self.downsample(x) + self.drop_path(self.mlp(self.norm2(x)))\r\n         return x\r\n \r\n if __name__ == '__main__':\r\n-    # input = paddle.randn((4, 96, 56, 56),dtype=\"float32\")\r\n-    # model = EfficientBlock(96, 192, 3, [1, 1, 1], [7, 5], 7, 7, 4, 2)\r\n-    # print(model(input).size())\r\n+    input = paddle.randn((4, 96, 56, 56),dtype=\"float32\")\r\n+    model = EfficientBlock(96, 192, 3, [1, 1, 1], [7, 5], 7, 7, 4, 2)\r\n+    print(model(input).size())\r\n     \r\n \r\n-    import paddle\r\n-    print(paddle.__version__)\r\n-    print(paddle.version.cuda())\r\n-    print(paddle.device.get_cudnn_version())\r\n-    import paddle.fluid\r\n-    paddle.fluid.install_check.run_check()\r\n-    exit()\r\n-from typing import List\r\n-\r\n-import math\r\n-from functools import partial\r\n-\r\n-import paddle\r\n-from paddle import nn\r\n-import paddle.nn.functional as F\r\n-from paddle.autograd import PyLayer\r\n-\r\n-##Swish激活函数  由之前的激活函数复合而成出来的   \r\n-##通过创建 PyLayer 子类的方式实现Python端自定义算子\r\n-class SwishImplementation(PyLayer):\r\n-    def forward(ctx, i):\r\n-        result = i * F.sigmoid(i)\r\n-        ctx.save_for_backward(i)\r\n-        return result\r\n-\r\n-    def backward(ctx, grad_output):\r\n-        i = ctx.saved_variables[0]\r\n-        sigmoid_i = F.sigmoid(i)\r\n-        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\r\n-\r\n-class MemoryEfficientSwish(nn.Layer):\r\n-    def forward(self, x):\r\n-        return SwishImplementation.apply(x)\r\n-\r\n-\r\n-def drop_path(x, drop_prob=0., training=False):\r\n-    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\r\n-    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\r\n-    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ...\r\n-    \"\"\"\r\n-    if drop_prob == 0. or not training:\r\n-        return x\r\n-    keep_prob = paddle.to_tensor(1 - drop_prob)\r\n-    shape = (paddle.shape(x)[0], ) + (1, ) * (x.ndim - 1)\r\n-    random_tensor = keep_prob + paddle.rand(shape, dtype=x.dtype)\r\n-    random_tensor = paddle.floor(random_tensor)  # binarize\r\n-    output = x.divide(keep_prob) * random_tensor\r\n-    return output\r\n-\r\n-\r\n-class DropPath(nn.Layer):\r\n-    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\r\n-    \"\"\"\r\n-\r\n-    def __init__(self, drop_prob=None):\r\n-        super(DropPath, self).__init__()\r\n-        self.drop_prob = drop_prob\r\n-\r\n-    def forward(self, x):\r\n-        return drop_path(x, self.drop_prob, self.training)\r\n-    \r\n-    def extra_repr(self):\r\n-        return f'drop_prob={round(self.drop_prob,3):0.3f}'\r\n-\r\n-\r\n-\r\n-class AttnMap(nn.Layer):\r\n-    def __init__(self, dim):\r\n-        super().__init__()\r\n-        self.act_block = nn.Sequential(\r\n-                            nn.Conv2D(dim, dim, 1, 1, 0),\r\n-                            MemoryEfficientSwish(),\r\n-                            nn.Conv2D(dim, dim, 1, 1, 0)\r\n-                            #nn.Identity()\r\n-                         )\r\n-    def forward(self, x):\r\n-        return self.act_block(x)\r\n-    \r\n-class EfficientAttention(nn.Layer):\r\n-\r\n-    def __init__(self, dim, num_heads, group_split: List[int], kernel_sizes: List[int], window_size=7, \r\n-                 attn_drop=0., proj_drop=0., qkv_bias=True):\r\n-        super().__init__()\r\n-        assert sum(group_split) == num_heads\r\n-        assert len(kernel_sizes) + 1 == len(group_split)\r\n-        self.dim = dim\r\n-        self.num_heads = num_heads\r\n-        self.dim_head = dim // num_heads\r\n-        self.scalor = self.dim_head ** -0.5\r\n-        self.kernel_sizes = kernel_sizes\r\n-        self.window_size = window_size\r\n-        self.group_split = group_split\r\n-        convs = []\r\n-        act_blocks = []\r\n-        qkvs = []\r\n-        #projs = []\r\n-        for i in range(len(kernel_sizes)):\r\n-            kernel_size = kernel_sizes[i]\r\n-            group_head = group_split[i]\r\n-            if group_head == 0:\r\n-                continue\r\n-            convs.append(nn.Conv2D(3*self.dim_head*group_head, 3*self.dim_head*group_head, kernel_size,\r\n-                         1, kernel_size//2, groups=3*self.dim_head*group_head))\r\n-            act_blocks.append(AttnMap(self.dim_head*group_head))\r\n-            qkvs.append(nn.Conv2D(dim, 3*group_head*self.dim_head, 1, 1, 0, bias_attr=qkv_bias))\r\n-            #projs.append(nn.Linear(group_head*self.dim_head, group_head*self.dim_head, bias=qkv_bias))\r\n-        if group_split[-1] != 0:\r\n-            self.global_q = nn.Conv2D(dim, group_split[-1]*self.dim_head, 1, 1, 0, bias_attr=qkv_bias)\r\n-            self.global_kv = nn.Conv2D(dim, group_split[-1]*self.dim_head*2, 1, 1, 0, bias_attr=qkv_bias)\r\n-            #self.global_proj = nn.Linear(group_split[-1]*self.dim_head, group_split[-1]*self.dim_head, bias=qkv_bias)\r\n-            self.avgpool = nn.AvgPool2D(window_size, window_size) if window_size!=1 else nn.Identity()\r\n-\r\n-        self.convs = nn.LayerList(convs)\r\n-        self.act_blocks = nn.LayerList(act_blocks)\r\n-        self.qkvs = nn.LayerList(qkvs)\r\n-        self.proj = nn.Conv2D(dim, dim, 1, 1, 0, bias_attr=qkv_bias)\r\n-        self.attn_drop = nn.Dropout(attn_drop)\r\n-        self.proj_drop = nn.Dropout(proj_drop)\r\n-\r\n-    def high_fre_attntion(self, x: paddle.Tensor, to_qkv: nn.Layer, mixer: nn.Layer, attn_block: nn.Layer):\r\n-        '''\r\n-        x: (b c h w)\r\n-        '''\r\n-        print(x.size)\r\n-        b, c, h, w = x.shape\r\n-        qkv = to_qkv(x) #(b (3 m d) h w)\r\n-        qkv = mixer(qkv).reshape(b, 3, -1, h, w).transpose(0, 1).contiguous() #(3 b (m d) h w)\r\n-        q, k, v = qkv #(b (m d) h w)\r\n-        attn = attn_block(q.mul(k)).mul(self.scalor)\r\n-        attn = self.attn_drop(paddle.tanh(attn))\r\n-        res = attn.mul(v) #(b (m d) h w)\r\n-        return res\r\n-        \r\n-    def low_fre_attention(self, x : paddle.Tensor, to_q: nn.layer, to_kv: nn.Layer, avgpool: nn.Layer):\r\n-        '''\r\n-        x: (b c h w)\r\n-        '''\r\n-        \r\n-        b, c, h, w = x.shape\r\n-        \r\n-        q = to_q(x).reshape(b, -1, self.dim_head, h*w).transpose(-1, -2).contiguous() #(b m (h w) d)\r\n-        kv = avgpool(x) #(b c h w)\r\n-        kv = to_kv(kv).view(b, 2, -1, self.dim_head, (h*w)//(self.window_size**2)).permute(1, 0, 2, 4, 3).contiguous() #(2 b m (H W) d)\r\n-        k, v = kv #(b m (H W) d)\r\n-        attn = self.scalor * q @ k.transpose(-1, -2) #(b m (h w) (H W))\r\n-        attn = self.attn_drop(attn.softmax(dim=-1))\r\n-        res = attn @ v #(b m (h w) d)\r\n-        res = res.transpose(2, 3).reshape(b, -1, h, w).contiguous()\r\n-        return res\r\n-\r\n-    def forward(self, x: paddle.Tensor):\r\n-        '''\r\n-        x: (b c h w)\r\n-        '''\r\n-        res = []\r\n-        for i in range(len(self.kernel_sizes)):\r\n-            if self.group_split[i] == 0:\r\n-                continue\r\n-            res.append(self.high_fre_attntion(x, self.qkvs[i], self.convs[i], self.act_blocks[i]))\r\n-        if self.group_split[-1] != 0:\r\n-            res.append(self.low_fre_attention(x, self.global_q, self.global_kv, self.avgpool))\r\n-        return self.proj_drop(self.proj(paddle.cat(res, dim=1)))\r\n-\r\n-\r\n-class ConvFFN(nn.Layer):\r\n-\r\n-    def __init__(self, in_channels, hidden_channels, kernel_size, stride,\r\n-                 out_channels, act_layer=nn.GELU, drop_out=0.):\r\n-        super().__init__()\r\n-        self.fc1 = nn.Conv2D(in_channels, hidden_channels, 1, 1, 0)\r\n-        self.act = act_layer()\r\n-        self.dwconv = nn.Conv2D(hidden_channels, hidden_channels, kernel_size, stride, \r\n-                                kernel_size//2, groups=hidden_channels)\r\n-        self.fc2 = nn.Conv2D(hidden_channels, out_channels, 1, 1, 0)\r\n-        self.drop = nn.Dropout(drop_out)\r\n-\r\n-    def forward(self, x: paddle.Tensor):\r\n-        '''\r\n-        x: (b h w c)\r\n-        '''\r\n-        x = self.fc1(x)\r\n-        x = self.act(x)\r\n-        x = self.dwconv(x)\r\n-        x = self.drop(x)\r\n-        x = self.fc2(x)\r\n-        x = self.drop(x)\r\n-        return x\r\n-\r\n-\r\n-\r\n-class EfficientBlock(nn.Layer):\r\n-\r\n-    def __init__(self, dim, out_dim, num_heads, group_split: List[int], kernel_sizes: List[int], window_size: int,\r\n-                 mlp_kernel_size: int, mlp_ratio: int, stride: int, attn_drop=0., mlp_drop=0., qkv_bias=True,\r\n-                 drop_path=0.):\r\n-        super().__init__()\r\n-        self.dim = dim\r\n-        self.mlp_ratio = mlp_ratio\r\n-        self.norm1 = nn.GroupNorm(1, dim)\r\n-        self.attn = EfficientAttention(dim, num_heads, group_split, kernel_sizes, window_size,\r\n-                                       attn_drop, mlp_drop, qkv_bias)\r\n-        self.drop_path = DropPath(drop_path)\r\n-        self.norm2 = nn.GroupNorm(1, dim)\r\n-        mlp_hidden_dim = int(dim * mlp_ratio)\r\n-        self.stride = stride\r\n-        if stride == 1:\r\n-            self.downsample = nn.Identity()\r\n-        else:\r\n-            self.downsample = nn.Sequential(\r\n-                                nn.Conv2D(dim, dim, mlp_kernel_size, 2, mlp_kernel_size//2),\r\n-                                nn.SyncBatchNorm(dim),\r\n-                                nn.Conv2D(dim, out_dim, 1, 1, 0),\r\n-                            )\r\n-        self.mlp = ConvFFN(dim, mlp_hidden_dim, mlp_kernel_size, stride, out_dim, \r\n-                        drop_out=mlp_drop)\r\n-    def forward(self, x: paddle.Tensor):\r\n-        x = x + self.drop_path(self.attn(self.norm1(x)))\r\n-        x = self.downsample(x) + self.drop_path(self.mlp(self.norm2(x)))\r\n-        return x\r\n-\r\n-if __name__ == '__main__':\r\n-    # input = paddle.randn((4, 96, 56, 56),dtype=\"float32\")\r\n-    # model = EfficientBlock(96, 192, 3, [1, 1, 1], [7, 5], 7, 7, 4, 2)\r\n-    # print(model(input).size())\r\n-    \r\n-\r\n-    import paddle\r\n-    print(paddle.__version__)\r\n-    print(paddle.device.get_device())\r\n-    print(paddle.device.get_cudnn_version())\r\n-    import paddle.fluid\r\n-    paddle.fluid.install_check.run_check()\r\n-    exit()\r\n+    # import paddle\r\n+    # print(paddle.__version__)\r\n+    # print(paddle.version.cuda())\r\n+    # print(paddle.device.get_cudnn_version())\r\n+    # import paddle.fluid\r\n+    # paddle.fluid.install_check.run_check()\r\n+    # exit()\r\n"
                },
                {
                    "date": 1709204150916,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -213,9 +213,9 @@\n \r\n if __name__ == '__main__':\r\n     input = paddle.randn((4, 96, 56, 56),dtype=\"float32\")\r\n     model = EfficientBlock(96, 192, 3, [1, 1, 1], [7, 5], 7, 7, 4, 2)\r\n-    print(model(input).size())\r\n+    print(model(input))\r\n     \r\n \r\n     # import paddle\r\n     # print(paddle.__version__)\r\n"
                }
            ],
            "date": 1709197351357,
            "name": "Commit-0",
            "content": "from timm.models.layers import DropPath\r\nfrom typing import List\r\n\r\nimport math\r\nfrom functools import partial\r\n\r\nimport paddle\r\nfrom paddle import nn\r\nimport paddle.nn.functional as F\r\nfrom paddle.autograd import PyLayer\r\n\r\n##Swish激活函数  由之前的激活函数复合而成出来的   \r\n##通过创建 PyLayer 子类的方式实现Python端自定义算子\r\nclass SwishImplementation(PyLayer):\r\n    def forward(ctx, i):\r\n        result = i * F.sigmoid(i)\r\n        ctx.save_for_backward(i)\r\n        return result\r\n\r\n    def backward(ctx, grad_output):\r\n        i = ctx.saved_variables[0]\r\n        sigmoid_i = F.sigmoid(i)\r\n        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\r\n\r\nclass MemoryEfficientSwish(nn.Layer):\r\n    def forward(self, x):\r\n        return SwishImplementation.apply(x)\r\n\r\n\r\n\r\nclass AttnMap(nn.Module):\r\n    def __init__(self, dim):\r\n        super().__init__()\r\n        self.act_block = nn.Sequential(\r\n                            nn.Conv2d(dim, dim, 1, 1, 0),\r\n                            MemoryEfficientSwish(),\r\n                            nn.Conv2d(dim, dim, 1, 1, 0)\r\n                            #nn.Identity()\r\n                         )\r\n    def forward(self, x):\r\n        return self.act_block(x)\r\n    \r\ndef drop_path(x, drop_prob=0., training=False):\r\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\r\n    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\r\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ...\r\n    \"\"\"\r\n    if drop_prob == 0. or not training:\r\n        return x\r\n    keep_prob = paddle.to_tensor(1 - drop_prob)\r\n    shape = (paddle.shape(x)[0], ) + (1, ) * (x.ndim - 1)\r\n    random_tensor = keep_prob + paddle.rand(shape, dtype=x.dtype)\r\n    random_tensor = paddle.floor(random_tensor)  # binarize\r\n    output = x.divide(keep_prob) * random_tensor\r\n    return output\r\n\r\n\r\nclass DropPath(nn.Layer):\r\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\r\n    \"\"\"\r\n\r\n    def __init__(self, drop_prob=None):\r\n        super(DropPath, self).__init__()\r\n        self.drop_prob = drop_prob\r\n\r\n    def forward(self, x):\r\n        return drop_path(x, self.drop_prob, self.training)\r\n    \r\n    def extra_repr(self):\r\n        return f'drop_prob={round(self.drop_prob,3):0.3f}'\r\n"
        }
    ]
}